# ğŸ”¢ FASE 2.1: Embedding Service

## Servicio de GeneraciÃ³n de Embeddings SemÃ¡nticos

**Documento:** 2.1-EMBEDDING-SERVICE.md
**Fase:** 2 - Embeddings SemÃ¡nticos
**DuraciÃ³n estimada:** 1.5-2 semanas
**Dependencias:** OpenAI API Key, Node.js 18+

---

## ğŸ“‹ Ãndice

1. [Objetivo](#objetivo)
2. [Arquitectura del Servicio](#arquitectura-del-servicio)
3. [SelecciÃ³n del Modelo](#selecciÃ³n-del-modelo)
4. [ImplementaciÃ³n](#implementaciÃ³n)
5. [Optimizaciones](#optimizaciones)
6. [Checklist de ImplementaciÃ³n](#checklist-de-implementaciÃ³n)

---

## ğŸ¯ Objetivo

Crear un servicio robusto para generar embeddings semÃ¡nticos de texto usando OpenAI API. Este servicio serÃ¡ la base para bÃºsqueda semÃ¡ntica, clasificaciÃ³n de patrones y detecciÃ³n de similitud.

### Casos de Uso

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDING USE CASES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. PATTERN DETECTION                                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ "Quiero     â”‚ â”€â”€â”€â–º â”‚  Embedding  â”‚ â”€â”€â”€â–º â”‚  Similar to â”‚ â”‚
â”‚     â”‚  reservar"  â”‚      â”‚  [0.1, 0.3] â”‚      â”‚  scheduling â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  2. SEMANTIC SEARCH                                            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ "horarios   â”‚ â”€â”€â”€â–º â”‚  Embedding  â”‚ â”€â”€â”€â–º â”‚  Top 5      â”‚ â”‚
â”‚     â”‚  de citas"  â”‚      â”‚  [0.2, 0.5] â”‚      â”‚  FAQ matchesâ”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  3. SIMILARITY CLUSTERING                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ Customer    â”‚ â”€â”€â”€â–º â”‚  Embeddings â”‚ â”€â”€â”€â–º â”‚  Cluster    â”‚ â”‚
â”‚     â”‚ Messages    â”‚      â”‚  Matrix     â”‚      â”‚  Analysis   â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  4. ANOMALY DETECTION                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚     â”‚ New Message â”‚ â”€â”€â”€â–º â”‚  Embedding  â”‚ â”€â”€â”€â–º â”‚  Distance   â”‚ â”‚
â”‚     â”‚             â”‚      â”‚  Comparison â”‚      â”‚  from norm  â”‚ â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Arquitectura del Servicio

### Componentes

```
src/features/ai/embeddings/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ embedding.service.ts        # ğŸ†• Servicio principal
â”‚   â”œâ”€â”€ embedding-cache.service.ts  # ğŸ†• Cache de embeddings
â”‚   â””â”€â”€ batch-processor.service.ts  # ğŸ†• Procesamiento en batch
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ openai-provider.ts          # ğŸ†• Proveedor OpenAI
â”‚   â””â”€â”€ provider.interface.ts       # ğŸ†• Interface para providers
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ text-preprocessor.ts        # ğŸ†• Limpieza de texto
â”‚   â””â”€â”€ vector-utils.ts             # ğŸ†• Operaciones vectoriales
â””â”€â”€ types/
    â””â”€â”€ embedding.types.ts          # ğŸ†• Tipos
```

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚     [Input Text]                                                   â”‚
â”‚          â”‚                                                          â”‚
â”‚          â–¼                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚  Text Preprocessor â”‚                                            â”‚
â”‚  â”‚  - Normalize       â”‚                                            â”‚
â”‚  â”‚  - Clean           â”‚                                            â”‚
â”‚  â”‚  - Truncate        â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚            â”‚                                                        â”‚
â”‚            â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Cache Check     â”‚â”€â”€â”€â”€â–ºâ”‚   Cache Hit?      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Return cached   â”‚                  â”‚
â”‚            â”‚ No             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚            â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   Batch Queue     â”‚  (agrupa requests)                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚            â”‚                                                        â”‚
â”‚            â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   OpenAI API      â”‚                                             â”‚
â”‚  â”‚   text-embedding  â”‚                                             â”‚
â”‚  â”‚   -3-small        â”‚                                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚            â”‚                                                        â”‚
â”‚            â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   Store in Cache  â”‚                                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚            â”‚                                                        â”‚
â”‚            â–¼                                                        â”‚
â”‚     [Embedding Vector]                                              â”‚
â”‚     [1536 dimensions]                                               â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  SelecciÃ³n del Modelo

### ComparaciÃ³n de Modelos OpenAI

| Modelo | Dimensiones | Max Tokens | Costo/1M tokens | Performance |
|--------|-------------|------------|-----------------|-------------|
| text-embedding-3-small | 1536 | 8191 | $0.02 | Bueno |
| text-embedding-3-large | 3072 | 8191 | $0.13 | Mejor |
| text-embedding-ada-002 | 1536 | 8191 | $0.10 | Legacy |

### RecomendaciÃ³n: `text-embedding-3-small`

**Razones:**
1. **Costo-efectivo**: 5x mÃ¡s barato que ada-002
2. **Dimensiones suficientes**: 1536 es adecuado para nuestros casos de uso
3. **Soporte de truncaciÃ³n**: Maneja textos largos automÃ¡ticamente
4. **Rendimiento**: Mejor que ada-002 en benchmarks recientes

### ConfiguraciÃ³n Recomendada

```typescript
const EMBEDDING_CONFIG = {
  model: 'text-embedding-3-small',
  dimensions: 1536, // MÃ¡ximo para este modelo
  maxInputTokens: 8191,
  encoding: 'cl100k_base', // Tokenizer usado por el modelo
};
```

---

## ğŸ’» ImplementaciÃ³n

### Archivo: `src/features/ai/embeddings/types/embedding.types.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - Embedding Types
// Tipos para el servicio de embeddings
// =====================================================

export interface EmbeddingConfig {
  model: string;
  dimensions: number;
  maxInputTokens: number;
  batchSize: number;
  cacheEnabled: boolean;
  cacheTTLSeconds: number;
}

export interface EmbeddingRequest {
  text: string;
  metadata?: Record<string, unknown>;
  skipCache?: boolean;
}

export interface EmbeddingResult {
  text: string;
  embedding: number[];
  dimensions: number;
  model: string;
  tokenCount: number;
  cached: boolean;
  processingTimeMs: number;
}

export interface BatchEmbeddingRequest {
  texts: string[];
  metadata?: Record<string, unknown>[];
}

export interface BatchEmbeddingResult {
  results: EmbeddingResult[];
  totalTokens: number;
  processingTimeMs: number;
  cacheHits: number;
  apiCalls: number;
}

export interface EmbeddingCacheEntry {
  text_hash: string;
  embedding: number[];
  model: string;
  created_at: string;
  expires_at: string;
  hit_count: number;
}

export interface SimilarityResult {
  text: string;
  score: number;
  metadata?: Record<string, unknown>;
}

// Vector operations
export type DistanceMetric = 'cosine' | 'euclidean' | 'dot_product';

export interface VectorSearchOptions {
  metric: DistanceMetric;
  limit: number;
  threshold?: number;
  includeMetadata?: boolean;
}
```

### Archivo: `src/features/ai/embeddings/providers/provider.interface.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - Embedding Provider Interface
// Interface abstracta para providers de embeddings
// =====================================================

import { EmbeddingResult, BatchEmbeddingResult } from '../types/embedding.types';

export interface EmbeddingProvider {
  name: string;
  model: string;
  dimensions: number;
  maxBatchSize: number;

  /**
   * Generate embedding for a single text
   */
  embed(text: string): Promise<EmbeddingResult>;

  /**
   * Generate embeddings for multiple texts
   */
  embedBatch(texts: string[]): Promise<BatchEmbeddingResult>;

  /**
   * Check if provider is available
   */
  isAvailable(): Promise<boolean>;

  /**
   * Get token count for text
   */
  getTokenCount(text: string): number;
}
```

### Archivo: `src/features/ai/embeddings/providers/openai-provider.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - OpenAI Embedding Provider
// Proveedor de embeddings usando OpenAI API
// =====================================================

import OpenAI from 'openai';
import { encoding_for_model } from 'tiktoken';
import {
  EmbeddingProvider,
} from './provider.interface';
import {
  EmbeddingResult,
  BatchEmbeddingResult,
} from '../types/embedding.types';

export class OpenAIEmbeddingProvider implements EmbeddingProvider {
  name = 'openai';
  model = 'text-embedding-3-small';
  dimensions = 1536;
  maxBatchSize = 100; // OpenAI limit is 2048, but we use conservative value

  private client: OpenAI;
  private encoder: ReturnType<typeof encoding_for_model>;

  constructor() {
    this.client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    // Initialize tokenizer
    this.encoder = encoding_for_model('text-embedding-3-small');
  }

  async embed(text: string): Promise<EmbeddingResult> {
    const startTime = Date.now();

    const tokenCount = this.getTokenCount(text);

    // Truncate if necessary
    const processedText = this.truncateToTokenLimit(text);

    try {
      const response = await this.client.embeddings.create({
        model: this.model,
        input: processedText,
        dimensions: this.dimensions,
      });

      return {
        text: processedText,
        embedding: response.data[0].embedding,
        dimensions: this.dimensions,
        model: this.model,
        tokenCount,
        cached: false,
        processingTimeMs: Date.now() - startTime,
      };
    } catch (error) {
      console.error('[OpenAI Provider] Embedding error:', error);
      throw new Error(`Failed to generate embedding: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  async embedBatch(texts: string[]): Promise<BatchEmbeddingResult> {
    const startTime = Date.now();

    // Process texts
    const processedTexts = texts.map(t => this.truncateToTokenLimit(t));
    const tokenCounts = processedTexts.map(t => this.getTokenCount(t));
    const totalTokens = tokenCounts.reduce((sum, count) => sum + count, 0);

    // Split into chunks if necessary
    const chunks: string[][] = [];
    for (let i = 0; i < processedTexts.length; i += this.maxBatchSize) {
      chunks.push(processedTexts.slice(i, i + this.maxBatchSize));
    }

    const results: EmbeddingResult[] = [];
    let apiCalls = 0;

    for (const chunk of chunks) {
      try {
        const response = await this.client.embeddings.create({
          model: this.model,
          input: chunk,
          dimensions: this.dimensions,
        });

        apiCalls++;

        for (let i = 0; i < response.data.length; i++) {
          results.push({
            text: chunk[i],
            embedding: response.data[i].embedding,
            dimensions: this.dimensions,
            model: this.model,
            tokenCount: tokenCounts[results.length],
            cached: false,
            processingTimeMs: 0, // Will be set at batch level
          });
        }
      } catch (error) {
        console.error('[OpenAI Provider] Batch embedding error:', error);
        throw error;
      }
    }

    const processingTimeMs = Date.now() - startTime;

    // Update processing time for each result
    results.forEach(r => {
      r.processingTimeMs = processingTimeMs / results.length;
    });

    return {
      results,
      totalTokens,
      processingTimeMs,
      cacheHits: 0, // No cache at provider level
      apiCalls,
    };
  }

  async isAvailable(): Promise<boolean> {
    try {
      // Simple check with minimal request
      await this.client.embeddings.create({
        model: this.model,
        input: 'test',
        dimensions: this.dimensions,
      });
      return true;
    } catch {
      return false;
    }
  }

  getTokenCount(text: string): number {
    try {
      return this.encoder.encode(text).length;
    } catch {
      // Fallback: estimate based on character count
      return Math.ceil(text.length / 4);
    }
  }

  private truncateToTokenLimit(text: string, maxTokens: number = 8000): string {
    const tokens = this.encoder.encode(text);

    if (tokens.length <= maxTokens) {
      return text;
    }

    // Truncate tokens and decode back to text
    const truncatedTokens = tokens.slice(0, maxTokens);
    return this.encoder.decode(truncatedTokens);
  }
}

// Export singleton
export const openAIProvider = new OpenAIEmbeddingProvider();
```

### Archivo: `src/features/ai/embeddings/utils/text-preprocessor.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - Text Preprocessor
// Utilidades para limpiar y normalizar texto
// =====================================================

export interface PreprocessOptions {
  lowercase?: boolean;
  removeUrls?: boolean;
  removeEmails?: boolean;
  removePhoneNumbers?: boolean;
  removeExtraWhitespace?: boolean;
  removeEmojis?: boolean;
  maxLength?: number;
}

const DEFAULT_OPTIONS: PreprocessOptions = {
  lowercase: false, // Preserve case for better semantic understanding
  removeUrls: true,
  removeEmails: true,
  removePhoneNumbers: false, // May be relevant context
  removeExtraWhitespace: true,
  removeEmojis: false, // Emojis can carry sentiment
  maxLength: 8000,
};

/**
 * Preprocess text for embedding generation
 */
export function preprocessText(text: string, options: PreprocessOptions = {}): string {
  const opts = { ...DEFAULT_OPTIONS, ...options };
  let processed = text;

  // Remove URLs
  if (opts.removeUrls) {
    processed = processed.replace(
      /https?:\/\/[^\s]+/g,
      '[URL]'
    );
  }

  // Remove emails
  if (opts.removeEmails) {
    processed = processed.replace(
      /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g,
      '[EMAIL]'
    );
  }

  // Remove phone numbers
  if (opts.removePhoneNumbers) {
    processed = processed.replace(
      /(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}/g,
      '[PHONE]'
    );
  }

  // Remove emojis
  if (opts.removeEmojis) {
    processed = processed.replace(
      /[\u{1F600}-\u{1F64F}]|[\u{1F300}-\u{1F5FF}]|[\u{1F680}-\u{1F6FF}]|[\u{1F1E0}-\u{1F1FF}]|[\u{2600}-\u{26FF}]|[\u{2700}-\u{27BF}]/gu,
      ''
    );
  }

  // Normalize whitespace
  if (opts.removeExtraWhitespace) {
    processed = processed.replace(/\s+/g, ' ').trim();
  }

  // Lowercase
  if (opts.lowercase) {
    processed = processed.toLowerCase();
  }

  // Truncate
  if (opts.maxLength && processed.length > opts.maxLength) {
    processed = processed.substring(0, opts.maxLength);
    // Try to end at a word boundary
    const lastSpace = processed.lastIndexOf(' ');
    if (lastSpace > opts.maxLength * 0.8) {
      processed = processed.substring(0, lastSpace);
    }
  }

  return processed;
}

/**
 * Generate a hash for text (for caching)
 */
export function hashText(text: string): string {
  let hash = 0;
  for (let i = 0; i < text.length; i++) {
    const char = text.charCodeAt(i);
    hash = ((hash << 5) - hash) + char;
    hash = hash & hash; // Convert to 32bit integer
  }
  return Math.abs(hash).toString(36);
}

/**
 * Chunk text into smaller pieces for batch processing
 */
export function chunkText(
  text: string,
  chunkSize: number = 1000,
  overlap: number = 100
): string[] {
  if (text.length <= chunkSize) {
    return [text];
  }

  const chunks: string[] = [];
  let start = 0;

  while (start < text.length) {
    let end = start + chunkSize;

    // Try to end at a sentence boundary
    if (end < text.length) {
      const sentenceEnd = text.substring(start, end + 100).search(/[.!?]\s/);
      if (sentenceEnd > chunkSize * 0.7) {
        end = start + sentenceEnd + 1;
      }
    }

    chunks.push(text.substring(start, end).trim());
    start = end - overlap;
  }

  return chunks;
}

/**
 * Combine chunks with their embeddings for search
 */
export interface TextChunk {
  text: string;
  startIndex: number;
  endIndex: number;
  embedding?: number[];
}

export function createTextChunks(
  text: string,
  chunkSize: number = 1000,
  overlap: number = 100
): TextChunk[] {
  const chunks = chunkText(text, chunkSize, overlap);
  let currentIndex = 0;

  return chunks.map((chunk) => {
    const startIndex = text.indexOf(chunk, currentIndex);
    const endIndex = startIndex + chunk.length;
    currentIndex = startIndex + 1;

    return {
      text: chunk,
      startIndex,
      endIndex,
    };
  });
}
```

### Archivo: `src/features/ai/embeddings/utils/vector-utils.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - Vector Utilities
// Operaciones matemÃ¡ticas para vectores de embeddings
// =====================================================

/**
 * Calculate cosine similarity between two vectors
 * Range: -1 to 1 (1 = identical, 0 = orthogonal, -1 = opposite)
 */
export function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have the same dimensions');
  }

  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }

  const denominator = Math.sqrt(normA) * Math.sqrt(normB);
  if (denominator === 0) return 0;

  return dotProduct / denominator;
}

/**
 * Calculate Euclidean distance between two vectors
 * Range: 0 to infinity (0 = identical)
 */
export function euclideanDistance(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have the same dimensions');
  }

  let sum = 0;
  for (let i = 0; i < a.length; i++) {
    const diff = a[i] - b[i];
    sum += diff * diff;
  }

  return Math.sqrt(sum);
}

/**
 * Calculate dot product between two vectors
 */
export function dotProduct(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have the same dimensions');
  }

  let result = 0;
  for (let i = 0; i < a.length; i++) {
    result += a[i] * b[i];
  }

  return result;
}

/**
 * Normalize a vector to unit length
 */
export function normalizeVector(vector: number[]): number[] {
  const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
  if (magnitude === 0) return vector;
  return vector.map(val => val / magnitude);
}

/**
 * Calculate average of multiple vectors
 */
export function averageVectors(vectors: number[][]): number[] {
  if (vectors.length === 0) return [];

  const dimensions = vectors[0].length;
  const result = new Array(dimensions).fill(0);

  for (const vector of vectors) {
    for (let i = 0; i < dimensions; i++) {
      result[i] += vector[i];
    }
  }

  return result.map(val => val / vectors.length);
}

/**
 * Find top K most similar vectors
 */
export function findTopK(
  query: number[],
  candidates: Array<{ embedding: number[]; metadata?: unknown }>,
  k: number,
  metric: 'cosine' | 'euclidean' | 'dot_product' = 'cosine'
): Array<{ index: number; score: number; metadata?: unknown }> {
  const scored = candidates.map((candidate, index) => {
    let score: number;

    switch (metric) {
      case 'cosine':
        score = cosineSimilarity(query, candidate.embedding);
        break;
      case 'euclidean':
        // Convert distance to similarity (smaller distance = higher similarity)
        score = 1 / (1 + euclideanDistance(query, candidate.embedding));
        break;
      case 'dot_product':
        score = dotProduct(query, candidate.embedding);
        break;
    }

    return { index, score, metadata: candidate.metadata };
  });

  // Sort by score descending and take top K
  return scored
    .sort((a, b) => b.score - a.score)
    .slice(0, k);
}

/**
 * Reduce vector dimensions using PCA-like approach (simple version)
 * For production, consider using a proper PCA library
 */
export function reduceVectorDimensions(
  vector: number[],
  targetDimensions: number
): number[] {
  if (vector.length <= targetDimensions) {
    return vector;
  }

  // Simple approach: average adjacent dimensions
  const ratio = vector.length / targetDimensions;
  const result: number[] = [];

  for (let i = 0; i < targetDimensions; i++) {
    const start = Math.floor(i * ratio);
    const end = Math.floor((i + 1) * ratio);
    let sum = 0;

    for (let j = start; j < end; j++) {
      sum += vector[j];
    }

    result.push(sum / (end - start));
  }

  return result;
}

/**
 * Quantize vector to reduce storage (lossy compression)
 */
export function quantizeVector(
  vector: number[],
  bits: 8 | 16 = 8
): { quantized: number[]; scale: number; offset: number } {
  const min = Math.min(...vector);
  const max = Math.max(...vector);
  const range = max - min;

  const maxValue = bits === 8 ? 255 : 65535;

  const scale = range / maxValue;
  const offset = min;

  const quantized = vector.map(val =>
    Math.round((val - offset) / scale)
  );

  return { quantized, scale, offset };
}

/**
 * Dequantize vector back to floats
 */
export function dequantizeVector(
  quantized: number[],
  scale: number,
  offset: number
): number[] {
  return quantized.map(val => val * scale + offset);
}
```

### Archivo: `src/features/ai/embeddings/services/embedding-cache.service.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - Embedding Cache Service
// Cache de embeddings para evitar llamadas repetidas
// =====================================================

import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { EmbeddingCacheEntry, EmbeddingResult } from '../types/embedding.types';
import { hashText } from '../utils/text-preprocessor';

interface CacheConfig {
  enabled: boolean;
  ttlSeconds: number;
  maxEntries: number;
  tableName: string;
}

const DEFAULT_CONFIG: CacheConfig = {
  enabled: true,
  ttlSeconds: 7 * 24 * 60 * 60, // 7 days
  maxEntries: 100000,
  tableName: 'ai_embedding_cache',
};

export class EmbeddingCacheService {
  private supabase: SupabaseClient;
  private config: CacheConfig;
  private memoryCache: Map<string, { embedding: number[]; expires: number }>;
  private memoryCacheMaxSize: number = 1000;

  constructor(config: Partial<CacheConfig> = {}) {
    this.supabase = createClient(
      process.env.NEXT_PUBLIC_SUPABASE_URL!,
      process.env.SUPABASE_SERVICE_ROLE_KEY!
    );
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.memoryCache = new Map();
  }

  /**
   * Get embedding from cache
   */
  async get(text: string, model: string): Promise<number[] | null> {
    if (!this.config.enabled) return null;

    const hash = this.generateHash(text, model);

    // Check memory cache first
    const memoryCached = this.memoryCache.get(hash);
    if (memoryCached && memoryCached.expires > Date.now()) {
      return memoryCached.embedding;
    }

    // Check database cache
    try {
      const { data, error } = await this.supabase
        .from(this.config.tableName)
        .select('embedding, expires_at')
        .eq('text_hash', hash)
        .eq('model', model)
        .single();

      if (error || !data) return null;

      // Check if expired
      if (new Date(data.expires_at) < new Date()) {
        // Delete expired entry
        await this.supabase
          .from(this.config.tableName)
          .delete()
          .eq('text_hash', hash);
        return null;
      }

      // Update hit count
      await this.supabase
        .from(this.config.tableName)
        .update({ hit_count: this.supabase.rpc('increment_hit_count') })
        .eq('text_hash', hash);

      // Add to memory cache
      this.addToMemoryCache(hash, data.embedding);

      return data.embedding;
    } catch (error) {
      console.error('[EmbeddingCache] Get error:', error);
      return null;
    }
  }

  /**
   * Store embedding in cache
   */
  async set(text: string, model: string, embedding: number[]): Promise<void> {
    if (!this.config.enabled) return;

    const hash = this.generateHash(text, model);
    const expiresAt = new Date(Date.now() + this.config.ttlSeconds * 1000);

    // Add to memory cache
    this.addToMemoryCache(hash, embedding);

    // Store in database
    try {
      await this.supabase
        .from(this.config.tableName)
        .upsert({
          text_hash: hash,
          embedding,
          model,
          text_preview: text.substring(0, 100), // Store preview for debugging
          created_at: new Date().toISOString(),
          expires_at: expiresAt.toISOString(),
          hit_count: 0,
        }, {
          onConflict: 'text_hash,model',
        });
    } catch (error) {
      console.error('[EmbeddingCache] Set error:', error);
    }
  }

  /**
   * Get multiple embeddings from cache
   */
  async getMany(
    texts: string[],
    model: string
  ): Promise<Map<string, number[] | null>> {
    const results = new Map<string, number[] | null>();
    const hashes = texts.map(t => this.generateHash(t, model));

    // Check memory cache
    const uncachedIndices: number[] = [];
    for (let i = 0; i < texts.length; i++) {
      const memoryCached = this.memoryCache.get(hashes[i]);
      if (memoryCached && memoryCached.expires > Date.now()) {
        results.set(texts[i], memoryCached.embedding);
      } else {
        results.set(texts[i], null);
        uncachedIndices.push(i);
      }
    }

    if (uncachedIndices.length === 0) {
      return results;
    }

    // Fetch from database
    const uncachedHashes = uncachedIndices.map(i => hashes[i]);

    try {
      const { data } = await this.supabase
        .from(this.config.tableName)
        .select('text_hash, embedding')
        .in('text_hash', uncachedHashes)
        .eq('model', model)
        .gt('expires_at', new Date().toISOString());

      if (data) {
        for (const entry of data) {
          const index = hashes.indexOf(entry.text_hash);
          if (index >= 0) {
            results.set(texts[index], entry.embedding);
            this.addToMemoryCache(entry.text_hash, entry.embedding);
          }
        }
      }
    } catch (error) {
      console.error('[EmbeddingCache] GetMany error:', error);
    }

    return results;
  }

  /**
   * Clear expired entries
   */
  async cleanup(): Promise<number> {
    try {
      const { count } = await this.supabase
        .from(this.config.tableName)
        .delete()
        .lt('expires_at', new Date().toISOString())
        .select('*', { count: 'exact', head: true });

      // Clear memory cache
      const now = Date.now();
      for (const [key, value] of this.memoryCache) {
        if (value.expires < now) {
          this.memoryCache.delete(key);
        }
      }

      return count ?? 0;
    } catch (error) {
      console.error('[EmbeddingCache] Cleanup error:', error);
      return 0;
    }
  }

  /**
   * Get cache statistics
   */
  async getStats(): Promise<{
    totalEntries: number;
    memoryEntries: number;
    averageHitCount: number;
    oldestEntry: string | null;
  }> {
    const { data, count } = await this.supabase
      .from(this.config.tableName)
      .select('hit_count, created_at', { count: 'exact' })
      .order('created_at', { ascending: true })
      .limit(1);

    const avgHits = await this.supabase
      .from(this.config.tableName)
      .select('hit_count')
      .then(({ data }) =>
        data ? data.reduce((sum, e) => sum + e.hit_count, 0) / data.length : 0
      );

    return {
      totalEntries: count ?? 0,
      memoryEntries: this.memoryCache.size,
      averageHitCount: avgHits,
      oldestEntry: data?.[0]?.created_at ?? null,
    };
  }

  // Private methods

  private generateHash(text: string, model: string): string {
    return `${model}:${hashText(text)}`;
  }

  private addToMemoryCache(hash: string, embedding: number[]): void {
    // Evict oldest entries if at capacity
    if (this.memoryCache.size >= this.memoryCacheMaxSize) {
      const firstKey = this.memoryCache.keys().next().value;
      if (firstKey) {
        this.memoryCache.delete(firstKey);
      }
    }

    this.memoryCache.set(hash, {
      embedding,
      expires: Date.now() + 5 * 60 * 1000, // 5 minutes memory cache
    });
  }
}

export const embeddingCache = new EmbeddingCacheService();
```

### Archivo: `src/features/ai/embeddings/services/embedding.service.ts`

```typescript
// =====================================================
// TIS TIS PLATFORM - Embedding Service
// Servicio principal para generaciÃ³n de embeddings
// =====================================================

import {
  EmbeddingConfig,
  EmbeddingRequest,
  EmbeddingResult,
  BatchEmbeddingRequest,
  BatchEmbeddingResult,
} from '../types/embedding.types';
import { EmbeddingProvider } from '../providers/provider.interface';
import { openAIProvider } from '../providers/openai-provider';
import { EmbeddingCacheService, embeddingCache } from './embedding-cache.service';
import { preprocessText } from '../utils/text-preprocessor';

const DEFAULT_CONFIG: EmbeddingConfig = {
  model: 'text-embedding-3-small',
  dimensions: 1536,
  maxInputTokens: 8191,
  batchSize: 50,
  cacheEnabled: true,
  cacheTTLSeconds: 7 * 24 * 60 * 60, // 7 days
};

export class EmbeddingService {
  private provider: EmbeddingProvider;
  private cache: EmbeddingCacheService;
  private config: EmbeddingConfig;

  constructor(
    config: Partial<EmbeddingConfig> = {},
    provider?: EmbeddingProvider,
    cache?: EmbeddingCacheService
  ) {
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.provider = provider ?? openAIProvider;
    this.cache = cache ?? embeddingCache;
  }

  /**
   * Generate embedding for a single text
   */
  async embed(request: EmbeddingRequest): Promise<EmbeddingResult> {
    const startTime = Date.now();

    // Preprocess text
    const processedText = preprocessText(request.text);

    // Check cache
    if (this.config.cacheEnabled && !request.skipCache) {
      const cached = await this.cache.get(processedText, this.config.model);
      if (cached) {
        return {
          text: processedText,
          embedding: cached,
          dimensions: this.config.dimensions,
          model: this.config.model,
          tokenCount: this.provider.getTokenCount(processedText),
          cached: true,
          processingTimeMs: Date.now() - startTime,
        };
      }
    }

    // Generate embedding
    const result = await this.provider.embed(processedText);

    // Store in cache
    if (this.config.cacheEnabled) {
      await this.cache.set(processedText, this.config.model, result.embedding);
    }

    return {
      ...result,
      processingTimeMs: Date.now() - startTime,
    };
  }

  /**
   * Generate embeddings for multiple texts
   */
  async embedBatch(request: BatchEmbeddingRequest): Promise<BatchEmbeddingResult> {
    const startTime = Date.now();

    // Preprocess texts
    const processedTexts = request.texts.map(t => preprocessText(t));

    // Check cache for all texts
    let cacheHits = 0;
    const cachedMap = this.config.cacheEnabled
      ? await this.cache.getMany(processedTexts, this.config.model)
      : new Map<string, number[] | null>();

    // Separate cached and uncached
    const uncachedTexts: string[] = [];
    const uncachedIndices: number[] = [];

    for (let i = 0; i < processedTexts.length; i++) {
      if (cachedMap.get(processedTexts[i])) {
        cacheHits++;
      } else {
        uncachedTexts.push(processedTexts[i]);
        uncachedIndices.push(i);
      }
    }

    // Generate embeddings for uncached texts
    let apiCalls = 0;
    const uncachedResults: EmbeddingResult[] = [];

    if (uncachedTexts.length > 0) {
      // Process in batches
      for (let i = 0; i < uncachedTexts.length; i += this.config.batchSize) {
        const batch = uncachedTexts.slice(i, i + this.config.batchSize);
        const batchResult = await this.provider.embedBatch(batch);

        apiCalls += batchResult.apiCalls;
        uncachedResults.push(...batchResult.results);

        // Store in cache
        if (this.config.cacheEnabled) {
          for (const result of batchResult.results) {
            await this.cache.set(result.text, this.config.model, result.embedding);
          }
        }
      }
    }

    // Combine results in original order
    const results: EmbeddingResult[] = [];
    let uncachedIndex = 0;

    for (let i = 0; i < processedTexts.length; i++) {
      const cached = cachedMap.get(processedTexts[i]);
      if (cached) {
        results.push({
          text: processedTexts[i],
          embedding: cached,
          dimensions: this.config.dimensions,
          model: this.config.model,
          tokenCount: this.provider.getTokenCount(processedTexts[i]),
          cached: true,
          processingTimeMs: 0,
        });
      } else {
        results.push({
          ...uncachedResults[uncachedIndex],
          cached: false,
        });
        uncachedIndex++;
      }
    }

    const processingTimeMs = Date.now() - startTime;
    const totalTokens = results.reduce((sum, r) => sum + r.tokenCount, 0);

    return {
      results,
      totalTokens,
      processingTimeMs,
      cacheHits,
      apiCalls,
    };
  }

  /**
   * Check if service is available
   */
  async isAvailable(): Promise<boolean> {
    return this.provider.isAvailable();
  }

  /**
   * Get token count for text
   */
  getTokenCount(text: string): number {
    return this.provider.getTokenCount(text);
  }

  /**
   * Get service configuration
   */
  getConfig(): EmbeddingConfig {
    return { ...this.config };
  }

  /**
   * Get cache statistics
   */
  async getCacheStats() {
    return this.cache.getStats();
  }

  /**
   * Clear expired cache entries
   */
  async cleanupCache(): Promise<number> {
    return this.cache.cleanup();
  }
}

// Export singleton instance
export const embeddingService = new EmbeddingService();
```

---

## âš¡ Optimizaciones

### 1. Batching AutomÃ¡tico

```typescript
// ConfiguraciÃ³n de batch Ã³ptimo
const BATCH_CONFIG = {
  maxBatchSize: 100,      // OpenAI limit
  maxWaitMs: 50,          // Max wait time before sending batch
  minBatchSize: 5,        // Min items before sending
};
```

### 2. Rate Limiting

```typescript
// Rate limiter para OpenAI API
import { RateLimiter } from 'limiter';

const rateLimiter = new RateLimiter({
  tokensPerInterval: 3000,  // Requests per minute
  interval: 'minute',
});

async function rateLimitedEmbed(text: string) {
  await rateLimiter.removeTokens(1);
  return embeddingService.embed({ text });
}
```

### 3. Fallback Provider

```typescript
// Fallback a provider alternativo
async function embedWithFallback(text: string): Promise<EmbeddingResult> {
  try {
    return await openAIProvider.embed(text);
  } catch (error) {
    console.warn('OpenAI failed, falling back to alternative');
    return await alternativeProvider.embed(text);
  }
}
```

---

## âœ… Checklist de ImplementaciÃ³n

```
â–¡ Paso 1: Setup
â”œâ”€â”€ [ ] Instalar dependencias (openai, tiktoken)
â”œâ”€â”€ [ ] Configurar OPENAI_API_KEY
â”œâ”€â”€ [ ] Crear estructura de carpetas
â””â”€â”€ [ ] Crear tipos base

â–¡ Paso 2: Implementar Provider
â”œâ”€â”€ [ ] Crear OpenAI provider
â”œâ”€â”€ [ ] Implementar single embed
â”œâ”€â”€ [ ] Implementar batch embed
â”œâ”€â”€ [ ] Agregar tokenization
â””â”€â”€ [ ] Unit tests

â–¡ Paso 3: Implementar Utilidades
â”œâ”€â”€ [ ] Text preprocessor
â”œâ”€â”€ [ ] Vector utils
â”œâ”€â”€ [ ] Hash function
â””â”€â”€ [ ] Unit tests

â–¡ Paso 4: Implementar Cache
â”œâ”€â”€ [ ] Crear tabla en Supabase
â”œâ”€â”€ [ ] Memory cache
â”œâ”€â”€ [ ] Database cache
â”œâ”€â”€ [ ] Cache cleanup
â””â”€â”€ [ ] Integration tests

â–¡ Paso 5: Implementar Service
â”œâ”€â”€ [ ] Crear EmbeddingService
â”œâ”€â”€ [ ] Integrar cache
â”œâ”€â”€ [ ] Batch processing
â”œâ”€â”€ [ ] Error handling
â””â”€â”€ [ ] E2E tests

â–¡ Paso 6: ValidaciÃ³n
â”œâ”€â”€ [ ] Test con textos reales
â”œâ”€â”€ [ ] Verificar costos
â”œâ”€â”€ [ ] Performance benchmark
â””â”€â”€ [ ] Documentar API
```

### SQL para tabla de cache

```sql
-- Migration: Create embedding cache table
CREATE TABLE IF NOT EXISTS ai_embedding_cache (
  text_hash TEXT NOT NULL,
  model TEXT NOT NULL,
  embedding VECTOR(1536) NOT NULL,
  text_preview TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  expires_at TIMESTAMPTZ NOT NULL,
  hit_count INTEGER DEFAULT 0,
  PRIMARY KEY (text_hash, model)
);

-- Index for expiration cleanup
CREATE INDEX idx_embedding_cache_expires ON ai_embedding_cache(expires_at);

-- Index for hit count analytics
CREATE INDEX idx_embedding_cache_hits ON ai_embedding_cache(hit_count DESC);

-- Function to increment hit count
CREATE OR REPLACE FUNCTION increment_hit_count()
RETURNS INTEGER AS $$
BEGIN
  RETURN hit_count + 1;
END;
$$ LANGUAGE plpgsql;
```

---

**Siguiente documento:** [2.2-VECTOR-STORE.md](./2.2-VECTOR-STORE.md)
