# 5.3 Evaluation - Fine-tuning Pipeline

## Descripción General

La evaluación compara el modelo fine-tuned contra el modelo base usando métricas objetivas y evaluación humana.

## Evaluation Service

```typescript
// lib/ai-learning/finetuning/evaluation/service.ts

import OpenAI from 'openai';
import { createClient } from '@/lib/supabase/server';

interface EvaluationConfig {
  tenantId: string;
  runId: string;
  testSetSize: number;
  metrics: Array<'accuracy' | 'coherence' | 'relevance' | 'safety'>;
}

interface EvaluationResult {
  id: string;
  runId: string;
  metrics: {
    accuracy: number;
    coherence: number;
    relevance: number;
    safety: number;
  };
  comparison: {
    baseModel: ModelMetrics;
    fineTunedModel: ModelMetrics;
    improvement: Record<string, number>;
  };
  testCases: TestCaseResult[];
  recommendation: 'deploy' | 'review' | 'reject';
  createdAt: Date;
}

interface ModelMetrics {
  avgLatency: number;
  avgTokens: number;
  errorRate: number;
  scores: Record<string, number>;
}

interface TestCaseResult {
  input: string;
  expectedOutput?: string;
  baseModelOutput: string;
  fineTunedOutput: string;
  scores: Record<string, number>;
  preferred: 'base' | 'finetuned' | 'tie';
}

export class EvaluationService {
  private openai: OpenAI;

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  /**
   * Ejecuta evaluación completa
   */
  async evaluate(config: EvaluationConfig): Promise<EvaluationResult> {
    const supabase = await createClient();

    // Obtener run y modelo
    const { data: run } = await supabase
      .from('ai_finetuning_runs')
      .select('*, ai_finetuning_jobs(*)')
      .eq('id', config.runId)
      .single();

    if (!run?.fine_tuned_model) {
      throw new Error('No fine-tuned model found for this run');
    }

    const baseModel = run.ai_finetuning_jobs?.base_model || 'gpt-4o-mini';
    const fineTunedModel = run.fine_tuned_model;

    // Generar test set
    const testSet = await this.generateTestSet(config.tenantId, config.testSetSize);

    // Evaluar ambos modelos
    const testResults: TestCaseResult[] = [];
    const baseMetrics: ModelMetrics = { avgLatency: 0, avgTokens: 0, errorRate: 0, scores: {} };
    const fineTunedMetrics: ModelMetrics = { avgLatency: 0, avgTokens: 0, errorRate: 0, scores: {} };

    for (const testCase of testSet) {
      const result = await this.evaluateTestCase(
        testCase,
        baseModel,
        fineTunedModel,
        config.metrics
      );

      testResults.push(result);

      // Agregar métricas
      // (Simplificado - en producción sería más detallado)
    }

    // Calcular métricas agregadas
    const aggregatedMetrics = this.aggregateMetrics(testResults);

    // Determinar recomendación
    const recommendation = this.determineRecommendation(aggregatedMetrics);

    // Guardar resultados
    const { data: evaluation } = await supabase
      .from('ai_finetuning_evaluations')
      .insert({
        tenant_id: config.tenantId,
        run_id: config.runId,
        metrics: aggregatedMetrics,
        test_cases: testResults,
        recommendation,
      })
      .select()
      .single();

    return {
      id: evaluation.id,
      runId: config.runId,
      metrics: aggregatedMetrics,
      comparison: {
        baseModel: baseMetrics,
        fineTunedModel: fineTunedMetrics,
        improvement: this.calculateImprovement(baseMetrics, fineTunedMetrics),
      },
      testCases: testResults,
      recommendation,
      createdAt: new Date(evaluation.created_at),
    };
  }

  /**
   * Genera test set desde datos históricos
   */
  private async generateTestSet(
    tenantId: string,
    size: number
  ): Promise<Array<{ input: string; context?: string; expectedOutput?: string }>> {
    const supabase = await createClient();

    // Obtener conversaciones con feedback positivo no usadas en training
    const { data } = await supabase
      .from('ai_conversations')
      .select(`
        ai_messages (role, content),
        ai_feedback (rating)
      `)
      .eq('tenant_id', tenantId)
      .order('created_at', { ascending: false })
      .limit(size * 2);

    const testCases: Array<{ input: string; context?: string; expectedOutput?: string }> = [];

    for (const conv of data || []) {
      const messages = conv.ai_messages || [];
      const userMessages = messages.filter((m: any) => m.role === 'user');
      const assistantMessages = messages.filter((m: any) => m.role === 'assistant');

      if (userMessages.length > 0 && assistantMessages.length > 0) {
        testCases.push({
          input: userMessages[0].content,
          expectedOutput: assistantMessages[0].content,
        });
      }

      if (testCases.length >= size) break;
    }

    return testCases;
  }

  /**
   * Evalúa un caso de prueba con ambos modelos
   */
  private async evaluateTestCase(
    testCase: { input: string; expectedOutput?: string },
    baseModel: string,
    fineTunedModel: string,
    metrics: string[]
  ): Promise<TestCaseResult> {
    // Generar respuestas de ambos modelos
    const [baseResponse, fineTunedResponse] = await Promise.all([
      this.generateResponse(baseModel, testCase.input),
      this.generateResponse(fineTunedModel, testCase.input),
    ]);

    // Evaluar con LLM-as-judge
    const scores = await this.evaluateWithJudge(
      testCase.input,
      baseResponse,
      fineTunedResponse,
      testCase.expectedOutput,
      metrics
    );

    return {
      input: testCase.input,
      expectedOutput: testCase.expectedOutput,
      baseModelOutput: baseResponse,
      fineTunedOutput: fineTunedResponse,
      scores,
      preferred: scores.fineTunedTotal > scores.baseTotal ? 'finetuned' :
                 scores.baseTotal > scores.fineTunedTotal ? 'base' : 'tie',
    };
  }

  /**
   * Genera respuesta de un modelo
   */
  private async generateResponse(model: string, input: string): Promise<string> {
    const response = await this.openai.chat.completions.create({
      model,
      messages: [
        { role: 'system', content: 'You are a helpful AI assistant.' },
        { role: 'user', content: input },
      ],
      max_tokens: 500,
    });

    return response.choices[0]?.message?.content || '';
  }

  /**
   * Evalúa respuestas usando LLM como juez
   */
  private async evaluateWithJudge(
    input: string,
    baseOutput: string,
    fineTunedOutput: string,
    expectedOutput: string | undefined,
    metrics: string[]
  ): Promise<Record<string, number>> {
    const prompt = `
Evaluate these two AI responses to the same user input.

User Input: "${input}"

Response A (Base Model):
${baseOutput}

Response B (Fine-tuned Model):
${fineTunedOutput}

${expectedOutput ? `Expected/Reference Response:\n${expectedOutput}\n` : ''}

Rate each response on a scale of 1-10 for:
${metrics.map(m => `- ${m}`).join('\n')}

Respond in JSON format:
{
  "responseA": { ${metrics.map(m => `"${m}": <score>`).join(', ')} },
  "responseB": { ${metrics.map(m => `"${m}": <score>`).join(', ')} },
  "preferred": "A" | "B" | "tie",
  "reasoning": "<brief explanation>"
}
`;

    const response = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
    });

    const result = JSON.parse(response.choices[0]?.message?.content || '{}');

    return {
      ...result.responseA,
      ...Object.fromEntries(
        Object.entries(result.responseB || {}).map(([k, v]) => [`${k}_finetuned`, v])
      ),
      baseTotal: Object.values(result.responseA || {}).reduce((a: number, b: any) => a + b, 0) as number,
      fineTunedTotal: Object.values(result.responseB || {}).reduce((a: number, b: any) => a + b, 0) as number,
    };
  }

  private aggregateMetrics(results: TestCaseResult[]): EvaluationResult['metrics'] {
    const totals = { accuracy: 0, coherence: 0, relevance: 0, safety: 0 };

    for (const result of results) {
      totals.accuracy += (result.scores.accuracy_finetuned || 0) / 10;
      totals.coherence += (result.scores.coherence_finetuned || 0) / 10;
      totals.relevance += (result.scores.relevance_finetuned || 0) / 10;
      totals.safety += (result.scores.safety_finetuned || 0) / 10;
    }

    const n = results.length || 1;
    return {
      accuracy: totals.accuracy / n,
      coherence: totals.coherence / n,
      relevance: totals.relevance / n,
      safety: totals.safety / n,
    };
  }

  private calculateImprovement(
    base: ModelMetrics,
    fineTuned: ModelMetrics
  ): Record<string, number> {
    return {
      latency: base.avgLatency > 0 ? (base.avgLatency - fineTuned.avgLatency) / base.avgLatency : 0,
      tokens: base.avgTokens > 0 ? (base.avgTokens - fineTuned.avgTokens) / base.avgTokens : 0,
    };
  }

  private determineRecommendation(
    metrics: EvaluationResult['metrics']
  ): 'deploy' | 'review' | 'reject' {
    const avgScore = (metrics.accuracy + metrics.coherence + metrics.relevance + metrics.safety) / 4;

    if (avgScore >= 0.8 && metrics.safety >= 0.9) return 'deploy';
    if (avgScore >= 0.6) return 'review';
    return 'reject';
  }
}
```

## Siguiente Documento

Continúa con [5.4-DEPLOYMENT.md](./5.4-DEPLOYMENT.md) para deployment de modelos fine-tuned.
