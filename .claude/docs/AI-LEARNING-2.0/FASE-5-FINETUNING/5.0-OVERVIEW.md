# ğŸ“ FASE 5: Fine-tuning

## Entrenamiento de Modelo Personalizado

**Documento:** 5.0-OVERVIEW.md
**Fase:** 5 - Fine-tuning
**DuraciÃ³n estimada:** 8-10 semanas
**Costo estimado:** $50-200 (one-time) + $25-50/mes hosting

---

## ğŸ“‹ Ãndice

1. [Objetivo](#objetivo)
2. [Estrategias de Fine-tuning](#estrategias-de-fine-tuning)
3. [Arquitectura](#arquitectura)
4. [Microfases](#microfases)
5. [Dataset Preparation](#dataset-preparation)
6. [Training Pipeline](#training-pipeline)
7. [Checklist General](#checklist-general)

---

## ğŸ¯ Objetivo

Entrenar un modelo de lenguaje personalizado usando datos de TIS TIS para mejorar la precisiÃ³n en tareas especÃ­ficas del dominio (dental, restaurantes) y reducir costos de API.

### Â¿Por quÃ© Fine-tuning?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINE-TUNING BENEFITS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  BEFORE (Generic Model)                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  User: "Quiero pedir hora para profilaxis"               â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Model: "Entiendo que deseas agendar una cita para       â”‚   â”‚
â”‚  â”‚          un procedimiento de limpieza dental..."         â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Issues:                                                 â”‚   â”‚
â”‚  â”‚  â€¢ Verbose response                                      â”‚   â”‚
â”‚  â”‚  â€¢ Generic tone                                          â”‚   â”‚
â”‚  â”‚  â€¢ Doesn't use clinic's terminology                      â”‚   â”‚
â”‚  â”‚  â€¢ Cost: $0.015 per request                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  AFTER (Fine-tuned Model)                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  User: "Quiero pedir hora para profilaxis"               â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Model: "Â¡Perfecto! Te agendo para profilaxis.           â”‚   â”‚
â”‚  â”‚          Â¿Prefieres maÃ±ana o tarde?"                     â”‚   â”‚
â”‚  â”‚                                                          â”‚   â”‚
â”‚  â”‚  Benefits:                                               â”‚   â”‚
â”‚  â”‚  â€¢ Concise, natural response âœ…                          â”‚   â”‚
â”‚  â”‚  â€¢ Uses domain terminology âœ…                            â”‚   â”‚
â”‚  â”‚  â€¢ Matches brand voice âœ…                                â”‚   â”‚
â”‚  â”‚  â€¢ Cost: $0.002 per request âœ…                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  ROI:                                                           â”‚
â”‚  â€¢ 87% cost reduction                                           â”‚
â”‚  â€¢ 15% improvement in satisfaction                              â”‚
â”‚  â€¢ Faster response times                                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Estrategias de Fine-tuning

### Opciones Disponibles

| Estrategia | Costo | Complejidad | Calidad | Recomendado |
|------------|-------|-------------|---------|-------------|
| OpenAI Fine-tuning | Medio | Bajo | Alto | âœ… Fase inicial |
| Llama 3 + LoRA | Bajo | Medio | Alto | Fase posterior |
| Full Fine-tuning | Alto | Alto | Muy Alto | Enterprise |
| RAG Enhanced | Bajo | Bajo | Medio | Complemento |

### RecomendaciÃ³n: OpenAI Fine-tuning

**Razones:**
1. **API simple**: FÃ¡cil de implementar
2. **Calidad garantizada**: Modelo base excelente
3. **Sin infraestructura**: No requiere GPUs
4. **IntegraciÃ³n nativa**: Mismo API que usamos

### ComparaciÃ³n de Modelos Base

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL COMPARISON                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  OpenAI GPT-4o-mini (recommended for fine-tuning)                  â”‚
â”‚  â”œâ”€â”€ Training: $3.00 / 1M tokens                                   â”‚
â”‚  â”œâ”€â”€ Inference: $0.15 / 1M input, $0.60 / 1M output               â”‚
â”‚  â”œâ”€â”€ Quality: Very High                                            â”‚
â”‚  â””â”€â”€ Best for: Production deployment                               â”‚
â”‚                                                                     â”‚
â”‚  OpenAI GPT-3.5-turbo (legacy)                                     â”‚
â”‚  â”œâ”€â”€ Training: $8.00 / 1M tokens                                   â”‚
â”‚  â”œâ”€â”€ Inference: $0.50 / 1M input, $1.50 / 1M output               â”‚
â”‚  â”œâ”€â”€ Quality: High                                                 â”‚
â”‚  â””â”€â”€ Best for: Cost optimization                                   â”‚
â”‚                                                                     â”‚
â”‚  Llama 3 70B (self-hosted)                                         â”‚
â”‚  â”œâ”€â”€ Training: GPU costs (~$100-500)                               â”‚
â”‚  â”œâ”€â”€ Inference: $0 (self-hosted)                                   â”‚
â”‚  â”œâ”€â”€ Quality: High                                                 â”‚
â”‚  â””â”€â”€ Best for: High volume, data privacy                           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Arquitectura

### Pipeline de Fine-tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINE-TUNING PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. DATA COLLECTION                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚Conversa- â”‚ â”‚RLHF      â”‚ â”‚User      â”‚ â”‚Corrected â”‚             â”‚
â”‚  â”‚tions     â”‚ â”‚Feedback  â”‚ â”‚Ratings   â”‚ â”‚Responses â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â”‚            â”‚            â”‚            â”‚                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                         â”‚                                          â”‚
â”‚                         â–¼                                          â”‚
â”‚  2. DATA PREPARATION                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Clean &      â”‚â”€â”€â”€â–ºâ”‚ Format to    â”‚â”€â”€â”€â–ºâ”‚ Split        â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ Deduplicate  â”‚    â”‚ JSONL        â”‚    â”‚ Train/Val    â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                          â”‚
â”‚                         â–¼                                          â”‚
â”‚  3. TRAINING                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Upload to    â”‚â”€â”€â”€â–ºâ”‚ Start        â”‚â”€â”€â”€â–ºâ”‚ Monitor      â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ OpenAI       â”‚    â”‚ Fine-tune    â”‚    â”‚ Progress     â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                          â”‚
â”‚                         â–¼                                          â”‚
â”‚  4. EVALUATION                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Benchmark    â”‚â”€â”€â”€â–ºâ”‚ A/B Test     â”‚â”€â”€â”€â–ºâ”‚ Quality      â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ on Test Set  â”‚    â”‚ in Staging   â”‚    â”‚ Metrics      â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                          â”‚
â”‚                         â–¼                                          â”‚
â”‚  5. DEPLOYMENT                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Gradual      â”‚â”€â”€â”€â–ºâ”‚ Monitor      â”‚â”€â”€â”€â–ºâ”‚ Full         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ Rollout      â”‚    â”‚ Performance  â”‚    â”‚ Deployment   â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Estructura de Archivos

```
src/features/ai/finetuning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data-collector.service.ts       # RecolecciÃ³n de datos
â”‚   â”œâ”€â”€ data-cleaner.service.ts         # Limpieza y validaciÃ³n
â”‚   â”œâ”€â”€ data-formatter.service.ts       # Formato JSONL
â”‚   â””â”€â”€ data-splitter.service.ts        # Split train/val/test
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ training-manager.service.ts     # GestiÃ³n de training jobs
â”‚   â”œâ”€â”€ openai-finetuning.service.ts    # IntegraciÃ³n OpenAI
â”‚   â””â”€â”€ hyperparameters.ts              # ConfiguraciÃ³n
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ benchmark.service.ts            # Benchmarks
â”‚   â”œâ”€â”€ metrics.service.ts              # MÃ©tricas de calidad
â”‚   â””â”€â”€ ab-test.service.ts              # A/B testing
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ model-registry.service.ts       # Registro de modelos
â”‚   â””â”€â”€ model-router.service.ts         # Routing entre modelos
â”œâ”€â”€ types/
â”‚   â””â”€â”€ finetuning.types.ts
â””â”€â”€ jobs/
    â””â”€â”€ training-pipeline.job.ts
```

---

## ğŸ“¦ Microfases

### 5.1 Data Preparation (2-3 semanas)

```
Objetivo: Recolectar y preparar dataset de entrenamiento

Entregables:
â”œâ”€â”€ DataCollectorService
â”œâ”€â”€ DataCleanerService
â”œâ”€â”€ DataFormatterService
â”œâ”€â”€ Quality filters
â””â”€â”€ Train/Val/Test splits

Dataset requirements:
â”œâ”€â”€ MÃ­nimo 1,000 ejemplos de alta calidad
â”œâ”€â”€ Balanceado por intent
â”œâ”€â”€ Incluye ejemplos negativos
â”œâ”€â”€ Diversidad de vocabulario
â””â”€â”€ Feedback positivo (RLHF)

Formato:
{
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```

### 5.2 Model Selection (1 semana)

```
Objetivo: Seleccionar y configurar modelo base

Entregables:
â”œâ”€â”€ Model comparison analysis
â”œâ”€â”€ Hyperparameter configuration
â”œâ”€â”€ Cost estimation
â””â”€â”€ Training plan

Consideraciones:
â”œâ”€â”€ TamaÃ±o del dataset vs modelo
â”œâ”€â”€ Budget disponible
â”œâ”€â”€ Latency requirements
â””â”€â”€ Quality targets
```

### 5.3 Training Pipeline (2-3 semanas)

```
Objetivo: Implementar pipeline de entrenamiento

Entregables:
â”œâ”€â”€ TrainingManagerService
â”œâ”€â”€ OpenAI integration
â”œâ”€â”€ Progress monitoring
â”œâ”€â”€ Error handling
â””â”€â”€ Job scheduling

Pipeline steps:
â”œâ”€â”€ Upload training file
â”œâ”€â”€ Validate format
â”œâ”€â”€ Create fine-tuning job
â”œâ”€â”€ Monitor progress
â”œâ”€â”€ Download fine-tuned model
â””â”€â”€ Register in model registry
```

### 5.4 Evaluation (1.5 semanas)

```
Objetivo: Evaluar calidad del modelo fine-tuned

Entregables:
â”œâ”€â”€ BenchmarkService
â”œâ”€â”€ MetricsService
â”œâ”€â”€ Comparison reports
â””â”€â”€ Quality thresholds

MÃ©tricas:
â”œâ”€â”€ Accuracy on test set
â”œâ”€â”€ BLEU/ROUGE scores
â”œâ”€â”€ Human evaluation
â”œâ”€â”€ A/B test results
â”œâ”€â”€ Cost comparison
â””â”€â”€ Latency comparison
```

### 5.5 Deployment (1.5 semanas)

```
Objetivo: Desplegar modelo en producciÃ³n

Entregables:
â”œâ”€â”€ ModelRegistryService
â”œâ”€â”€ ModelRouterService
â”œâ”€â”€ Gradual rollout system
â”œâ”€â”€ Rollback mechanism
â””â”€â”€ Monitoring dashboard

Deployment strategy:
â”œâ”€â”€ Shadow mode (log only)
â”œâ”€â”€ 10% traffic
â”œâ”€â”€ 50% traffic
â”œâ”€â”€ 100% traffic
â””â”€â”€ Monitor continuously
```

---

## ğŸ“Š Dataset Preparation

### Estructura del Dataset

```typescript
// Training example format
interface TrainingExample {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  // Optional metadata for filtering
  metadata?: {
    tenant_id: string;
    vertical: string;
    intent: string;
    satisfaction_score: number;
  };
}

// Example
const example: TrainingExample = {
  messages: [
    {
      role: "system",
      content: "Eres el asistente virtual de ClÃ­nica Dental Sonrisa. Ayudas a los pacientes con agendamiento de citas, informaciÃ³n sobre servicios y consultas generales. Responde de manera amigable, concisa y profesional."
    },
    {
      role: "user",
      content: "Hola, quiero pedir hora para limpieza dental"
    },
    {
      role: "assistant",
      content: "Â¡Hola! Con gusto te ayudo a agendar tu limpieza dental. Â¿Prefieres horario de maÃ±ana o tarde? Tenemos disponibilidad esta semana."
    }
  ],
  metadata: {
    tenant_id: "clinic_123",
    vertical: "dental",
    intent: "scheduling",
    satisfaction_score: 5
  }
};
```

### Data Quality Filters

```typescript
// Filters for high-quality training data
const QUALITY_FILTERS = {
  // Minimum satisfaction score (from RLHF)
  min_satisfaction: 4,

  // Message length bounds
  min_user_message_length: 5,
  max_user_message_length: 500,
  min_assistant_message_length: 10,
  max_assistant_message_length: 1000,

  // Exclude patterns
  exclude_patterns: [
    /error/i,
    /no entiendo/i,
    /no puedo ayudar/i,
  ],

  // Required patterns
  require_patterns: {
    scheduling: [/cita|hora|agendar|reservar/i],
    pricing: [/precio|costo|valor|cuÃ¡nto/i],
  },

  // Deduplication
  similarity_threshold: 0.9, // Remove near-duplicates
};
```

### Data Sources Priority

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SOURCES (Priority Order)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  1. USER-CORRECTED RESPONSES (Highest quality)                  â”‚
â”‚     Source: ai_rlhf_feedback.expected_response                  â”‚
â”‚     Quality: Very High (human-verified)                         â”‚
â”‚     Volume: Low (~5%)                                           â”‚
â”‚                                                                  â”‚
â”‚  2. POSITIVE RLHF FEEDBACK (High quality)                       â”‚
â”‚     Source: ai_rlhf_feedback WHERE feedback_type = 'positive'   â”‚
â”‚     Quality: High (user-approved)                               â”‚
â”‚     Volume: Medium (~30%)                                       â”‚
â”‚                                                                  â”‚
â”‚  3. HIGH-SATISFACTION CONVERSATIONS (Good quality)              â”‚
â”‚     Source: conversations with satisfaction_score >= 4          â”‚
â”‚     Quality: Good (implicit approval)                           â”‚
â”‚     Volume: High (~50%)                                         â”‚
â”‚                                                                  â”‚
â”‚  4. SYNTHETIC DATA (Augmentation)                               â”‚
â”‚     Source: Generated variations of high-quality examples       â”‚
â”‚     Quality: Medium (machine-generated)                         â”‚
â”‚     Volume: Variable (~15%)                                     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Training Pipeline

### OpenAI Fine-tuning Configuration

```typescript
// Hyperparameters for fine-tuning
const FINETUNING_CONFIG = {
  // Model selection
  base_model: "gpt-4o-mini-2024-07-18",

  // Training parameters
  hyperparameters: {
    n_epochs: 3,                    // Number of training epochs
    batch_size: "auto",             // Let OpenAI decide
    learning_rate_multiplier: 1.0,  // Default learning rate
  },

  // Validation
  validation_file: true,            // Include validation set
  validation_split: 0.1,            // 10% for validation

  // Suffix for model name
  suffix: "tistis-dental-v1",
};

// Estimated costs for 10K examples
const COST_ESTIMATE = {
  training_tokens: 5_000_000,       // ~500 tokens per example
  cost_per_million: 3.00,           // GPT-4o-mini training
  estimated_cost: 15.00,            // Total training cost
  inference_savings: "~80%",        // Compared to base model
};
```

### Training Script

```typescript
// scripts/finetune.ts
import OpenAI from 'openai';
import { readFileSync } from 'fs';

async function startFineTuning() {
  const openai = new OpenAI();

  // 1. Upload training file
  const trainingFile = await openai.files.create({
    file: readFileSync('data/training.jsonl'),
    purpose: 'fine-tune',
  });

  console.log('Training file uploaded:', trainingFile.id);

  // 2. Upload validation file
  const validationFile = await openai.files.create({
    file: readFileSync('data/validation.jsonl'),
    purpose: 'fine-tune',
  });

  console.log('Validation file uploaded:', validationFile.id);

  // 3. Create fine-tuning job
  const job = await openai.fineTuning.jobs.create({
    training_file: trainingFile.id,
    validation_file: validationFile.id,
    model: 'gpt-4o-mini-2024-07-18',
    hyperparameters: {
      n_epochs: 3,
    },
    suffix: 'tistis-dental-v1',
  });

  console.log('Fine-tuning job created:', job.id);

  // 4. Monitor progress
  while (true) {
    const status = await openai.fineTuning.jobs.retrieve(job.id);
    console.log(`Status: ${status.status}`);

    if (status.status === 'succeeded') {
      console.log('Fine-tuned model:', status.fine_tuned_model);
      break;
    } else if (status.status === 'failed') {
      console.error('Training failed:', status.error);
      break;
    }

    await new Promise(r => setTimeout(r, 60000)); // Wait 1 minute
  }
}

startFineTuning();
```

---

## âœ… Checklist General

```
â–¡ FASE 5.1: Data Preparation
â”œâ”€â”€ [ ] Identify data sources
â”œâ”€â”€ [ ] Implement data collector
â”œâ”€â”€ [ ] Implement quality filters
â”œâ”€â”€ [ ] Format to JSONL
â”œâ”€â”€ [ ] Split train/val/test
â”œâ”€â”€ [ ] Validate dataset
â””â”€â”€ [ ] Minimum 1000 examples

â–¡ FASE 5.2: Model Selection
â”œâ”€â”€ [ ] Compare models
â”œâ”€â”€ [ ] Estimate costs
â”œâ”€â”€ [ ] Configure hyperparameters
â”œâ”€â”€ [ ] Document decisions
â””â”€â”€ [ ] Get team approval

â–¡ FASE 5.3: Training Pipeline
â”œâ”€â”€ [ ] TrainingManager service
â”œâ”€â”€ [ ] OpenAI integration
â”œâ”€â”€ [ ] Progress monitoring
â”œâ”€â”€ [ ] Error handling
â”œâ”€â”€ [ ] Job scheduling
â””â”€â”€ [ ] Integration tests

â–¡ FASE 5.4: Evaluation
â”œâ”€â”€ [ ] Benchmark service
â”œâ”€â”€ [ ] Metrics service
â”œâ”€â”€ [ ] Test set evaluation
â”œâ”€â”€ [ ] Human evaluation
â”œâ”€â”€ [ ] A/B testing setup
â””â”€â”€ [ ] Quality gates

â–¡ FASE 5.5: Deployment
â”œâ”€â”€ [ ] Model registry
â”œâ”€â”€ [ ] Model router
â”œâ”€â”€ [ ] Gradual rollout
â”œâ”€â”€ [ ] Rollback mechanism
â”œâ”€â”€ [ ] Monitoring
â””â”€â”€ [ ] Documentation
```

---

## ğŸ“š Documentos Detallados

- [5.1-DATA-PREPARATION.md](./5.1-DATA-PREPARATION.md) ğŸ“ Pendiente
- [5.2-MODEL-SELECTION.md](./5.2-MODEL-SELECTION.md) ğŸ“ Pendiente
- [5.3-TRAINING-PIPELINE.md](./5.3-TRAINING-PIPELINE.md) ğŸ“ Pendiente
- [5.4-EVALUATION.md](./5.4-EVALUATION.md) ğŸ“ Pendiente
- [5.5-DEPLOYMENT.md](./5.5-DEPLOYMENT.md) ğŸ“ Pendiente

---

**Siguiente fase:** [FASE-6-XAI/6.0-OVERVIEW.md](../FASE-6-XAI/6.0-OVERVIEW.md)
