# 6.1 Decision Logger - Explainability (XAI)

## Descripción General

El Decision Logger captura y almacena información detallada sobre cada decisión del sistema de AI, incluyendo inputs, reasoning, outputs, y contexto.

## Schema

```sql
-- Logs de decisiones AI
CREATE TABLE ai_decision_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id),

  -- Contexto
  conversation_id UUID,
  message_id UUID,
  user_id UUID,

  -- Tipo de decisión
  decision_type TEXT NOT NULL, -- 'intent_classification' | 'response_generation' | 'escalation' | 'action'

  -- Input
  input_text TEXT,
  input_embedding vector(1536),
  input_features JSONB,

  -- Proceso de decisión
  model_used TEXT NOT NULL,
  prompt_template TEXT,
  prompt_rendered TEXT,

  -- Candidatos considerados
  candidates JSONB, -- [{option, score, reasoning}]

  -- Decisión final
  decision TEXT NOT NULL,
  confidence FLOAT NOT NULL,
  reasoning TEXT,

  -- Factores de influencia
  influence_factors JSONB, -- [{factor, weight, value}]

  -- Metadata
  latency_ms INT,
  tokens_used INT,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_decision_logs_tenant ON ai_decision_logs(tenant_id, created_at DESC);
CREATE INDEX idx_decision_logs_conversation ON ai_decision_logs(conversation_id);
CREATE INDEX idx_decision_logs_type ON ai_decision_logs(tenant_id, decision_type);
```

## Decision Logger Service

```typescript
// lib/ai-learning/xai/decision-logger.ts

import { createClient } from '@/lib/supabase/server';

interface DecisionContext {
  tenantId: string;
  conversationId?: string;
  messageId?: string;
  userId?: string;
}

interface Candidate {
  option: string;
  score: number;
  reasoning?: string;
}

interface InfluenceFactor {
  factor: string;
  weight: number;
  value: any;
  contribution: number;
}

interface DecisionLog {
  id: string;
  decisionType: string;
  inputText?: string;
  inputFeatures?: Record<string, any>;
  modelUsed: string;
  promptTemplate?: string;
  promptRendered?: string;
  candidates?: Candidate[];
  decision: string;
  confidence: number;
  reasoning?: string;
  influenceFactors?: InfluenceFactor[];
  latencyMs?: number;
  tokensUsed?: number;
  createdAt: Date;
}

export class DecisionLogger {
  private supabase: any;

  constructor() {
    // Inicializado lazy
  }

  private async getClient() {
    if (!this.supabase) {
      this.supabase = await createClient();
    }
    return this.supabase;
  }

  /**
   * Log de clasificación de intent
   */
  async logIntentClassification(
    context: DecisionContext,
    input: {
      text: string;
      embedding?: number[];
      features?: Record<string, any>;
    },
    candidates: Candidate[],
    decision: {
      intent: string;
      confidence: number;
    },
    metadata: {
      modelUsed: string;
      latencyMs?: number;
    }
  ): Promise<string> {
    const supabase = await this.getClient();

    const { data, error } = await supabase
      .from('ai_decision_logs')
      .insert({
        tenant_id: context.tenantId,
        conversation_id: context.conversationId,
        message_id: context.messageId,
        user_id: context.userId,
        decision_type: 'intent_classification',
        input_text: input.text,
        input_embedding: input.embedding ? `[${input.embedding.join(',')}]` : null,
        input_features: input.features,
        model_used: metadata.modelUsed,
        candidates,
        decision: decision.intent,
        confidence: decision.confidence,
        reasoning: this.generateIntentReasoning(candidates, decision),
        influence_factors: this.extractInfluenceFactors(candidates),
        latency_ms: metadata.latencyMs,
      })
      .select('id')
      .single();

    if (error) throw error;
    return data.id;
  }

  /**
   * Log de generación de respuesta
   */
  async logResponseGeneration(
    context: DecisionContext,
    input: {
      userMessage: string;
      conversationHistory: Array<{ role: string; content: string }>;
      detectedIntent?: string;
    },
    prompt: {
      template: string;
      rendered: string;
    },
    output: {
      response: string;
      confidence: number;
      tokensUsed: number;
    },
    metadata: {
      modelUsed: string;
      latencyMs?: number;
    }
  ): Promise<string> {
    const supabase = await this.getClient();

    const { data, error } = await supabase
      .from('ai_decision_logs')
      .insert({
        tenant_id: context.tenantId,
        conversation_id: context.conversationId,
        message_id: context.messageId,
        user_id: context.userId,
        decision_type: 'response_generation',
        input_text: input.userMessage,
        input_features: {
          history_length: input.conversationHistory.length,
          detected_intent: input.detectedIntent,
        },
        model_used: metadata.modelUsed,
        prompt_template: prompt.template,
        prompt_rendered: prompt.rendered,
        decision: output.response,
        confidence: output.confidence,
        tokens_used: output.tokensUsed,
        latency_ms: metadata.latencyMs,
      })
      .select('id')
      .single();

    if (error) throw error;
    return data.id;
  }

  /**
   * Log de decisión de escalación
   */
  async logEscalationDecision(
    context: DecisionContext,
    factors: InfluenceFactor[],
    decision: {
      shouldEscalate: boolean;
      reason: string;
      confidence: number;
    },
    metadata: {
      modelUsed: string;
      latencyMs?: number;
    }
  ): Promise<string> {
    const supabase = await this.getClient();

    const { data, error } = await supabase
      .from('ai_decision_logs')
      .insert({
        tenant_id: context.tenantId,
        conversation_id: context.conversationId,
        user_id: context.userId,
        decision_type: 'escalation',
        model_used: metadata.modelUsed,
        decision: decision.shouldEscalate ? 'escalate' : 'continue',
        confidence: decision.confidence,
        reasoning: decision.reason,
        influence_factors: factors,
        latency_ms: metadata.latencyMs,
      })
      .select('id')
      .single();

    if (error) throw error;
    return data.id;
  }

  /**
   * Obtiene logs de decisión para una conversación
   */
  async getConversationDecisions(conversationId: string): Promise<DecisionLog[]> {
    const supabase = await this.getClient();

    const { data } = await supabase
      .from('ai_decision_logs')
      .select('*')
      .eq('conversation_id', conversationId)
      .order('created_at', { ascending: true });

    return (data || []).map(this.mapLog);
  }

  /**
   * Obtiene estadísticas de decisiones
   */
  async getDecisionStats(
    tenantId: string,
    timeRange: { start: Date; end: Date }
  ): Promise<{
    totalDecisions: number;
    byType: Record<string, number>;
    avgConfidence: number;
    avgLatency: number;
  }> {
    const supabase = await this.getClient();

    const { data } = await supabase
      .from('ai_decision_logs')
      .select('decision_type, confidence, latency_ms')
      .eq('tenant_id', tenantId)
      .gte('created_at', timeRange.start.toISOString())
      .lt('created_at', timeRange.end.toISOString());

    const logs = data || [];
    const byType: Record<string, number> = {};

    let totalConfidence = 0;
    let totalLatency = 0;
    let latencyCount = 0;

    for (const log of logs) {
      byType[log.decision_type] = (byType[log.decision_type] || 0) + 1;
      totalConfidence += log.confidence || 0;
      if (log.latency_ms) {
        totalLatency += log.latency_ms;
        latencyCount++;
      }
    }

    return {
      totalDecisions: logs.length,
      byType,
      avgConfidence: logs.length > 0 ? totalConfidence / logs.length : 0,
      avgLatency: latencyCount > 0 ? totalLatency / latencyCount : 0,
    };
  }

  // Helpers
  private generateIntentReasoning(candidates: Candidate[], decision: { intent: string; confidence: number }): string {
    const topCandidates = candidates.slice(0, 3);
    const parts = [`Selected "${decision.intent}" with ${(decision.confidence * 100).toFixed(1)}% confidence.`];

    if (topCandidates.length > 1) {
      parts.push(`Alternatives considered: ${topCandidates.slice(1).map(c => `${c.option} (${(c.score * 100).toFixed(1)}%)`).join(', ')}`);
    }

    return parts.join(' ');
  }

  private extractInfluenceFactors(candidates: Candidate[]): InfluenceFactor[] {
    return candidates.map(c => ({
      factor: c.option,
      weight: 1,
      value: c.score,
      contribution: c.score,
    }));
  }

  private mapLog(row: any): DecisionLog {
    return {
      id: row.id,
      decisionType: row.decision_type,
      inputText: row.input_text,
      inputFeatures: row.input_features,
      modelUsed: row.model_used,
      promptTemplate: row.prompt_template,
      promptRendered: row.prompt_rendered,
      candidates: row.candidates,
      decision: row.decision,
      confidence: row.confidence,
      reasoning: row.reasoning,
      influenceFactors: row.influence_factors,
      latencyMs: row.latency_ms,
      tokensUsed: row.tokens_used,
      createdAt: new Date(row.created_at),
    };
  }
}
```

## Siguiente Documento

Continúa con [6.2-EVIDENCE-EXTRACTOR.md](./6.2-EVIDENCE-EXTRACTOR.md) para extracción de evidencia.
