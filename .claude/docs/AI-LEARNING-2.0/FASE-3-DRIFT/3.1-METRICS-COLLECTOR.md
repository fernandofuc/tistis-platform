# 3.1 Metrics Collector - Drift Detection

## Descripción General

El Metrics Collector es el componente central que captura y almacena todas las métricas necesarias para detectar drift en el sistema de AI Learning. Recopila datos de distribución de inputs, outputs, y métricas de rendimiento de forma continua.

## Arquitectura

```
┌─────────────────────────────────────────────────────────────────┐
│                     METRICS COLLECTOR                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │   Input      │  │   Output     │  │  Performance │          │
│  │   Collector  │  │   Collector  │  │   Collector  │          │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘          │
│         │                 │                 │                   │
│         └────────────┬────┴────────────────┘                   │
│                      │                                          │
│              ┌───────▼───────┐                                 │
│              │   Aggregator  │                                 │
│              └───────┬───────┘                                 │
│                      │                                          │
│              ┌───────▼───────┐                                 │
│              │   Time Series │                                 │
│              │    Storage    │                                 │
│              └───────────────┘                                 │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Schema de Base de Datos

```sql
-- =====================================================
-- TABLAS PARA MÉTRICAS DE DRIFT
-- =====================================================

-- Snapshots de distribución de inputs
CREATE TABLE ai_drift_input_snapshots (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id),

  -- Período del snapshot
  period_start TIMESTAMPTZ NOT NULL,
  period_end TIMESTAMPTZ NOT NULL,
  granularity TEXT NOT NULL DEFAULT 'hourly', -- 'hourly' | 'daily' | 'weekly'

  -- Métricas de texto
  avg_message_length FLOAT NOT NULL,
  std_message_length FLOAT NOT NULL,
  min_message_length INT NOT NULL,
  max_message_length INT NOT NULL,

  -- Distribución de longitudes (histograma)
  length_histogram JSONB NOT NULL, -- {"0-50": 100, "51-100": 200, ...}

  -- Métricas de vocabulario
  unique_tokens INT NOT NULL,
  vocabulary_size INT NOT NULL,
  oov_rate FLOAT NOT NULL, -- Out of vocabulary rate

  -- Distribución de tokens top
  top_tokens JSONB NOT NULL, -- [{"token": "cita", "count": 500}, ...]

  -- Distribución de intents detectados
  intent_distribution JSONB NOT NULL, -- {"scheduling": 0.4, "pricing": 0.2, ...}

  -- Métricas de embeddings (centroides)
  embedding_centroid vector(1536),
  embedding_spread FLOAT, -- Desviación estándar promedio

  -- Metadata
  message_count INT NOT NULL,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

  UNIQUE(tenant_id, period_start, granularity)
);

-- Snapshots de distribución de outputs
CREATE TABLE ai_drift_output_snapshots (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id),

  -- Período
  period_start TIMESTAMPTZ NOT NULL,
  period_end TIMESTAMPTZ NOT NULL,
  granularity TEXT NOT NULL DEFAULT 'hourly',

  -- Métricas de respuesta
  avg_response_length FLOAT NOT NULL,
  std_response_length FLOAT NOT NULL,

  -- Distribución de acciones tomadas
  action_distribution JSONB NOT NULL, -- {"book_appointment": 0.3, "answer_question": 0.5, ...}

  -- Distribución de confianza
  avg_confidence FLOAT NOT NULL,
  confidence_histogram JSONB NOT NULL, -- {"0.0-0.2": 10, "0.2-0.4": 20, ...}

  -- Tasa de escalación
  escalation_rate FLOAT NOT NULL,

  -- Tokens de respuesta
  avg_tokens_used INT NOT NULL,
  total_tokens_used BIGINT NOT NULL,

  -- Metadata
  response_count INT NOT NULL,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

  UNIQUE(tenant_id, period_start, granularity)
);

-- Snapshots de métricas de rendimiento
CREATE TABLE ai_drift_performance_snapshots (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id),

  -- Período
  period_start TIMESTAMPTZ NOT NULL,
  period_end TIMESTAMPTZ NOT NULL,
  granularity TEXT NOT NULL DEFAULT 'hourly',

  -- Métricas de satisfacción
  positive_feedback_rate FLOAT NOT NULL,
  negative_feedback_rate FLOAT NOT NULL,
  no_feedback_rate FLOAT NOT NULL,

  -- Métricas de resolución
  resolution_rate FLOAT NOT NULL, -- Conversaciones resueltas sin escalación
  avg_turns_to_resolution FLOAT NOT NULL,

  -- Métricas de latencia
  avg_response_time_ms FLOAT NOT NULL,
  p50_response_time_ms FLOAT NOT NULL,
  p95_response_time_ms FLOAT NOT NULL,
  p99_response_time_ms FLOAT NOT NULL,

  -- Métricas de error
  error_rate FLOAT NOT NULL,
  timeout_rate FLOAT NOT NULL,

  -- Métricas de uso
  total_conversations INT NOT NULL,
  total_messages INT NOT NULL,
  unique_users INT NOT NULL,

  -- Metadata
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

  UNIQUE(tenant_id, period_start, granularity)
);

-- Baseline de referencia para comparación
CREATE TABLE ai_drift_baselines (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL REFERENCES tenants(id),

  -- Tipo de baseline
  metric_type TEXT NOT NULL, -- 'input' | 'output' | 'performance'

  -- Período de referencia
  baseline_start TIMESTAMPTZ NOT NULL,
  baseline_end TIMESTAMPTZ NOT NULL,

  -- Datos del baseline (agregados del período)
  baseline_data JSONB NOT NULL,

  -- Estado
  is_active BOOLEAN NOT NULL DEFAULT true,

  -- Metadata
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  created_by UUID REFERENCES users(id),
  notes TEXT,

  UNIQUE(tenant_id, metric_type, is_active) WHERE is_active = true
);

-- Índices
CREATE INDEX idx_drift_input_tenant_period ON ai_drift_input_snapshots(tenant_id, period_start DESC);
CREATE INDEX idx_drift_output_tenant_period ON ai_drift_output_snapshots(tenant_id, period_start DESC);
CREATE INDEX idx_drift_perf_tenant_period ON ai_drift_performance_snapshots(tenant_id, period_start DESC);
CREATE INDEX idx_drift_baselines_tenant ON ai_drift_baselines(tenant_id, metric_type) WHERE is_active = true;

-- RLS Policies
ALTER TABLE ai_drift_input_snapshots ENABLE ROW LEVEL SECURITY;
ALTER TABLE ai_drift_output_snapshots ENABLE ROW LEVEL SECURITY;
ALTER TABLE ai_drift_performance_snapshots ENABLE ROW LEVEL SECURITY;
ALTER TABLE ai_drift_baselines ENABLE ROW LEVEL SECURITY;

CREATE POLICY tenant_isolation_input ON ai_drift_input_snapshots
  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::uuid);

CREATE POLICY tenant_isolation_output ON ai_drift_output_snapshots
  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::uuid);

CREATE POLICY tenant_isolation_perf ON ai_drift_performance_snapshots
  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::uuid);

CREATE POLICY tenant_isolation_baselines ON ai_drift_baselines
  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::uuid);
```

## Tipos TypeScript

```typescript
// lib/ai-learning/drift/types.ts

export interface InputSnapshot {
  id: string;
  tenantId: string;
  periodStart: Date;
  periodEnd: Date;
  granularity: 'hourly' | 'daily' | 'weekly';

  // Text metrics
  avgMessageLength: number;
  stdMessageLength: number;
  minMessageLength: number;
  maxMessageLength: number;
  lengthHistogram: Record<string, number>;

  // Vocabulary metrics
  uniqueTokens: number;
  vocabularySize: number;
  oovRate: number;
  topTokens: Array<{ token: string; count: number }>;

  // Intent distribution
  intentDistribution: Record<string, number>;

  // Embedding metrics
  embeddingCentroid?: number[];
  embeddingSpread?: number;

  messageCount: number;
  createdAt: Date;
}

export interface OutputSnapshot {
  id: string;
  tenantId: string;
  periodStart: Date;
  periodEnd: Date;
  granularity: 'hourly' | 'daily' | 'weekly';

  // Response metrics
  avgResponseLength: number;
  stdResponseLength: number;

  // Action distribution
  actionDistribution: Record<string, number>;

  // Confidence metrics
  avgConfidence: number;
  confidenceHistogram: Record<string, number>;

  // Escalation
  escalationRate: number;

  // Token usage
  avgTokensUsed: number;
  totalTokensUsed: number;

  responseCount: number;
  createdAt: Date;
}

export interface PerformanceSnapshot {
  id: string;
  tenantId: string;
  periodStart: Date;
  periodEnd: Date;
  granularity: 'hourly' | 'daily' | 'weekly';

  // Feedback metrics
  positiveFeedbackRate: number;
  negativeFeedbackRate: number;
  noFeedbackRate: number;

  // Resolution metrics
  resolutionRate: number;
  avgTurnsToResolution: number;

  // Latency metrics
  avgResponseTimeMs: number;
  p50ResponseTimeMs: number;
  p95ResponseTimeMs: number;
  p99ResponseTimeMs: number;

  // Error metrics
  errorRate: number;
  timeoutRate: number;

  // Usage metrics
  totalConversations: number;
  totalMessages: number;
  uniqueUsers: number;

  createdAt: Date;
}

export interface DriftBaseline {
  id: string;
  tenantId: string;
  metricType: 'input' | 'output' | 'performance';
  baselineStart: Date;
  baselineEnd: Date;
  baselineData: InputSnapshot | OutputSnapshot | PerformanceSnapshot;
  isActive: boolean;
  createdAt: Date;
  createdBy?: string;
  notes?: string;
}

export interface CollectorConfig {
  tenantId: string;
  granularity: 'hourly' | 'daily' | 'weekly';

  // Feature flags
  collectEmbeddings: boolean;
  collectTokens: boolean;

  // Sampling (para alto volumen)
  samplingRate: number; // 0.0 - 1.0

  // Retention
  retentionDays: number;
}
```

## Input Collector

```typescript
// lib/ai-learning/drift/collectors/input-collector.ts

import { createClient } from '@/lib/supabase/server';
import { EmbeddingService } from '../../embeddings/embedding-service';
import { Tokenizer } from '../../utils/tokenizer';
import type { InputSnapshot, CollectorConfig } from '../types';

interface RawMessage {
  id: string;
  content: string;
  metadata: Record<string, any>;
  createdAt: Date;
}

interface TokenStats {
  uniqueTokens: number;
  vocabularySize: number;
  topTokens: Array<{ token: string; count: number }>;
  oovRate: number;
}

export class InputCollector {
  private embeddingService: EmbeddingService;
  private tokenizer: Tokenizer;

  constructor() {
    this.embeddingService = new EmbeddingService();
    this.tokenizer = new Tokenizer();
  }

  /**
   * Recopila métricas de inputs para un período
   */
  async collectForPeriod(
    config: CollectorConfig,
    periodStart: Date,
    periodEnd: Date
  ): Promise<InputSnapshot | null> {
    const supabase = await createClient();

    // Obtener mensajes del período
    const { data: messages, error } = await supabase
      .from('ai_messages')
      .select('id, content, metadata, created_at')
      .eq('tenant_id', config.tenantId)
      .eq('role', 'user')
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString())
      .order('created_at', { ascending: true });

    if (error) {
      console.error('Error fetching messages for drift:', error);
      throw error;
    }

    if (!messages || messages.length === 0) {
      return null; // No hay datos para este período
    }

    // Aplicar sampling si es necesario
    const sampledMessages = this.applySampling(messages, config.samplingRate);

    // Calcular métricas
    const lengthMetrics = this.calculateLengthMetrics(sampledMessages);
    const tokenStats = config.collectTokens
      ? await this.calculateTokenStats(sampledMessages)
      : this.getDefaultTokenStats();
    const intentDistribution = this.extractIntentDistribution(sampledMessages);

    // Calcular embedding centroid si está habilitado
    let embeddingCentroid: number[] | undefined;
    let embeddingSpread: number | undefined;

    if (config.collectEmbeddings && sampledMessages.length > 0) {
      const embeddingMetrics = await this.calculateEmbeddingMetrics(
        sampledMessages.slice(0, 1000) // Limitar para no saturar
      );
      embeddingCentroid = embeddingMetrics.centroid;
      embeddingSpread = embeddingMetrics.spread;
    }

    const snapshot: Omit<InputSnapshot, 'id' | 'createdAt'> = {
      tenantId: config.tenantId,
      periodStart,
      periodEnd,
      granularity: config.granularity,

      // Length metrics
      avgMessageLength: lengthMetrics.avg,
      stdMessageLength: lengthMetrics.std,
      minMessageLength: lengthMetrics.min,
      maxMessageLength: lengthMetrics.max,
      lengthHistogram: lengthMetrics.histogram,

      // Token metrics
      uniqueTokens: tokenStats.uniqueTokens,
      vocabularySize: tokenStats.vocabularySize,
      oovRate: tokenStats.oovRate,
      topTokens: tokenStats.topTokens,

      // Intent distribution
      intentDistribution,

      // Embedding metrics
      embeddingCentroid,
      embeddingSpread,

      messageCount: messages.length, // Total, no sampled
    };

    // Guardar snapshot
    const { data: saved, error: saveError } = await supabase
      .from('ai_drift_input_snapshots')
      .upsert({
        tenant_id: snapshot.tenantId,
        period_start: snapshot.periodStart.toISOString(),
        period_end: snapshot.periodEnd.toISOString(),
        granularity: snapshot.granularity,
        avg_message_length: snapshot.avgMessageLength,
        std_message_length: snapshot.stdMessageLength,
        min_message_length: snapshot.minMessageLength,
        max_message_length: snapshot.maxMessageLength,
        length_histogram: snapshot.lengthHistogram,
        unique_tokens: snapshot.uniqueTokens,
        vocabulary_size: snapshot.vocabularySize,
        oov_rate: snapshot.oovRate,
        top_tokens: snapshot.topTokens,
        intent_distribution: snapshot.intentDistribution,
        embedding_centroid: embeddingCentroid ? `[${embeddingCentroid.join(',')}]` : null,
        embedding_spread: embeddingSpread,
        message_count: snapshot.messageCount,
      }, {
        onConflict: 'tenant_id,period_start,granularity'
      })
      .select()
      .single();

    if (saveError) {
      console.error('Error saving input snapshot:', saveError);
      throw saveError;
    }

    return {
      ...snapshot,
      id: saved.id,
      createdAt: new Date(saved.created_at),
    };
  }

  /**
   * Aplica sampling aleatorio a los mensajes
   */
  private applySampling(
    messages: RawMessage[],
    samplingRate: number
  ): RawMessage[] {
    if (samplingRate >= 1.0) return messages;

    return messages.filter(() => Math.random() < samplingRate);
  }

  /**
   * Calcula métricas de longitud de mensajes
   */
  private calculateLengthMetrics(messages: RawMessage[]): {
    avg: number;
    std: number;
    min: number;
    max: number;
    histogram: Record<string, number>;
  } {
    if (messages.length === 0) {
      return { avg: 0, std: 0, min: 0, max: 0, histogram: {} };
    }

    const lengths = messages.map(m => m.content.length);

    const avg = lengths.reduce((a, b) => a + b, 0) / lengths.length;
    const variance = lengths.reduce((sum, l) => sum + Math.pow(l - avg, 2), 0) / lengths.length;
    const std = Math.sqrt(variance);
    const min = Math.min(...lengths);
    const max = Math.max(...lengths);

    // Crear histograma con buckets de 50 caracteres
    const histogram: Record<string, number> = {};
    const bucketSize = 50;

    for (const length of lengths) {
      const bucketStart = Math.floor(length / bucketSize) * bucketSize;
      const bucketKey = `${bucketStart}-${bucketStart + bucketSize - 1}`;
      histogram[bucketKey] = (histogram[bucketKey] || 0) + 1;
    }

    return { avg, std, min, max, histogram };
  }

  /**
   * Calcula estadísticas de tokens
   */
  private async calculateTokenStats(messages: RawMessage[]): Promise<TokenStats> {
    const tokenCounts: Map<string, number> = new Map();
    let totalTokens = 0;

    for (const message of messages) {
      const tokens = this.tokenizer.tokenize(message.content.toLowerCase());

      for (const token of tokens) {
        tokenCounts.set(token, (tokenCounts.get(token) || 0) + 1);
        totalTokens++;
      }
    }

    // Top 100 tokens
    const sortedTokens = Array.from(tokenCounts.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, 100)
      .map(([token, count]) => ({ token, count }));

    // TODO: Calcular OOV rate contra vocabulario base
    const oovRate = 0; // Requiere vocabulario de referencia

    return {
      uniqueTokens: tokenCounts.size,
      vocabularySize: totalTokens,
      topTokens: sortedTokens,
      oovRate,
    };
  }

  private getDefaultTokenStats(): TokenStats {
    return {
      uniqueTokens: 0,
      vocabularySize: 0,
      topTokens: [],
      oovRate: 0,
    };
  }

  /**
   * Extrae distribución de intents de los metadatos
   */
  private extractIntentDistribution(
    messages: RawMessage[]
  ): Record<string, number> {
    const intentCounts: Map<string, number> = new Map();
    let totalWithIntent = 0;

    for (const message of messages) {
      const intent = message.metadata?.detected_intent ||
                     message.metadata?.intent ||
                     'unknown';

      intentCounts.set(intent, (intentCounts.get(intent) || 0) + 1);
      totalWithIntent++;
    }

    // Convertir a proporciones
    const distribution: Record<string, number> = {};

    for (const [intent, count] of intentCounts) {
      distribution[intent] = count / totalWithIntent;
    }

    return distribution;
  }

  /**
   * Calcula métricas de embeddings (centroide y spread)
   */
  private async calculateEmbeddingMetrics(
    messages: RawMessage[]
  ): Promise<{ centroid: number[]; spread: number }> {
    // Obtener embeddings en batch
    const texts = messages.map(m => m.content);
    const embeddings = await this.embeddingService.embedBatch({
      texts,
      tenantId: messages[0]?.metadata?.tenant_id || '',
    });

    if (embeddings.embeddings.length === 0) {
      return { centroid: [], spread: 0 };
    }

    const dimension = embeddings.embeddings[0].length;

    // Calcular centroide (promedio de todos los embeddings)
    const centroid = new Array(dimension).fill(0);

    for (const embedding of embeddings.embeddings) {
      for (let i = 0; i < dimension; i++) {
        centroid[i] += embedding[i];
      }
    }

    for (let i = 0; i < dimension; i++) {
      centroid[i] /= embeddings.embeddings.length;
    }

    // Calcular spread (desviación estándar promedio de distancia al centroide)
    let totalVariance = 0;

    for (const embedding of embeddings.embeddings) {
      let distance = 0;
      for (let i = 0; i < dimension; i++) {
        distance += Math.pow(embedding[i] - centroid[i], 2);
      }
      totalVariance += distance;
    }

    const spread = Math.sqrt(totalVariance / embeddings.embeddings.length);

    return { centroid, spread };
  }
}
```

## Output Collector

```typescript
// lib/ai-learning/drift/collectors/output-collector.ts

import { createClient } from '@/lib/supabase/server';
import type { OutputSnapshot, CollectorConfig } from '../types';

interface RawResponse {
  id: string;
  content: string;
  metadata: Record<string, any>;
  tokensUsed: number;
  createdAt: Date;
}

export class OutputCollector {
  /**
   * Recopila métricas de outputs para un período
   */
  async collectForPeriod(
    config: CollectorConfig,
    periodStart: Date,
    periodEnd: Date
  ): Promise<OutputSnapshot | null> {
    const supabase = await createClient();

    // Obtener respuestas del período
    const { data: responses, error } = await supabase
      .from('ai_messages')
      .select(`
        id,
        content,
        metadata,
        tokens_used,
        created_at
      `)
      .eq('tenant_id', config.tenantId)
      .eq('role', 'assistant')
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString());

    if (error) {
      console.error('Error fetching responses for drift:', error);
      throw error;
    }

    if (!responses || responses.length === 0) {
      return null;
    }

    // Calcular métricas
    const lengthMetrics = this.calculateLengthMetrics(responses);
    const actionDistribution = this.extractActionDistribution(responses);
    const confidenceMetrics = this.calculateConfidenceMetrics(responses);
    const escalationRate = this.calculateEscalationRate(responses);
    const tokenMetrics = this.calculateTokenMetrics(responses);

    const snapshot: Omit<OutputSnapshot, 'id' | 'createdAt'> = {
      tenantId: config.tenantId,
      periodStart,
      periodEnd,
      granularity: config.granularity,

      avgResponseLength: lengthMetrics.avg,
      stdResponseLength: lengthMetrics.std,

      actionDistribution,

      avgConfidence: confidenceMetrics.avg,
      confidenceHistogram: confidenceMetrics.histogram,

      escalationRate,

      avgTokensUsed: tokenMetrics.avg,
      totalTokensUsed: tokenMetrics.total,

      responseCount: responses.length,
    };

    // Guardar
    const { data: saved, error: saveError } = await supabase
      .from('ai_drift_output_snapshots')
      .upsert({
        tenant_id: snapshot.tenantId,
        period_start: snapshot.periodStart.toISOString(),
        period_end: snapshot.periodEnd.toISOString(),
        granularity: snapshot.granularity,
        avg_response_length: snapshot.avgResponseLength,
        std_response_length: snapshot.stdResponseLength,
        action_distribution: snapshot.actionDistribution,
        avg_confidence: snapshot.avgConfidence,
        confidence_histogram: snapshot.confidenceHistogram,
        escalation_rate: snapshot.escalationRate,
        avg_tokens_used: snapshot.avgTokensUsed,
        total_tokens_used: snapshot.totalTokensUsed,
        response_count: snapshot.responseCount,
      }, {
        onConflict: 'tenant_id,period_start,granularity'
      })
      .select()
      .single();

    if (saveError) {
      console.error('Error saving output snapshot:', saveError);
      throw saveError;
    }

    return {
      ...snapshot,
      id: saved.id,
      createdAt: new Date(saved.created_at),
    };
  }

  private calculateLengthMetrics(responses: RawResponse[]): {
    avg: number;
    std: number;
  } {
    const lengths = responses.map(r => r.content.length);
    const avg = lengths.reduce((a, b) => a + b, 0) / lengths.length;
    const variance = lengths.reduce((sum, l) => sum + Math.pow(l - avg, 2), 0) / lengths.length;

    return { avg, std: Math.sqrt(variance) };
  }

  private extractActionDistribution(
    responses: RawResponse[]
  ): Record<string, number> {
    const actionCounts: Map<string, number> = new Map();

    for (const response of responses) {
      const action = response.metadata?.action ||
                     response.metadata?.intent_handled ||
                     'general_response';

      actionCounts.set(action, (actionCounts.get(action) || 0) + 1);
    }

    const distribution: Record<string, number> = {};
    for (const [action, count] of actionCounts) {
      distribution[action] = count / responses.length;
    }

    return distribution;
  }

  private calculateConfidenceMetrics(responses: RawResponse[]): {
    avg: number;
    histogram: Record<string, number>;
  } {
    const confidences = responses
      .map(r => r.metadata?.confidence)
      .filter((c): c is number => typeof c === 'number');

    if (confidences.length === 0) {
      return { avg: 0, histogram: {} };
    }

    const avg = confidences.reduce((a, b) => a + b, 0) / confidences.length;

    // Histograma con buckets de 0.2
    const histogram: Record<string, number> = {};
    for (const conf of confidences) {
      const bucketStart = Math.floor(conf * 5) / 5;
      const bucketKey = `${bucketStart.toFixed(1)}-${(bucketStart + 0.2).toFixed(1)}`;
      histogram[bucketKey] = (histogram[bucketKey] || 0) + 1;
    }

    return { avg, histogram };
  }

  private calculateEscalationRate(responses: RawResponse[]): number {
    const escalated = responses.filter(r =>
      r.metadata?.escalated === true ||
      r.metadata?.requires_human === true
    ).length;

    return escalated / responses.length;
  }

  private calculateTokenMetrics(responses: RawResponse[]): {
    avg: number;
    total: number;
  } {
    const tokens = responses.map(r => r.tokensUsed || 0);
    const total = tokens.reduce((a, b) => a + b, 0);

    return {
      avg: total / responses.length,
      total,
    };
  }
}
```

## Performance Collector

```typescript
// lib/ai-learning/drift/collectors/performance-collector.ts

import { createClient } from '@/lib/supabase/server';
import type { PerformanceSnapshot, CollectorConfig } from '../types';

export class PerformanceCollector {
  /**
   * Recopila métricas de rendimiento para un período
   */
  async collectForPeriod(
    config: CollectorConfig,
    periodStart: Date,
    periodEnd: Date
  ): Promise<PerformanceSnapshot | null> {
    const supabase = await createClient();

    // Obtener feedback del período
    const { data: feedbackData } = await supabase
      .from('ai_feedback')
      .select('rating, created_at')
      .eq('tenant_id', config.tenantId)
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString());

    // Obtener conversaciones del período
    const { data: conversations } = await supabase
      .from('ai_conversations')
      .select(`
        id,
        status,
        metadata,
        created_at,
        updated_at
      `)
      .eq('tenant_id', config.tenantId)
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString());

    // Obtener métricas de latencia
    const { data: latencyData } = await supabase
      .from('ai_messages')
      .select('metadata, created_at')
      .eq('tenant_id', config.tenantId)
      .eq('role', 'assistant')
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString());

    // Obtener mensajes totales
    const { data: messagesData } = await supabase
      .from('ai_messages')
      .select('id, conversation_id, created_at')
      .eq('tenant_id', config.tenantId)
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString());

    // Obtener usuarios únicos
    const { data: usersData } = await supabase
      .from('ai_conversations')
      .select('user_id')
      .eq('tenant_id', config.tenantId)
      .gte('created_at', periodStart.toISOString())
      .lt('created_at', periodEnd.toISOString());

    if (!conversations || conversations.length === 0) {
      return null;
    }

    // Calcular métricas de feedback
    const feedbackMetrics = this.calculateFeedbackMetrics(
      feedbackData || [],
      (messagesData || []).length
    );

    // Calcular métricas de resolución
    const resolutionMetrics = this.calculateResolutionMetrics(
      conversations,
      messagesData || []
    );

    // Calcular métricas de latencia
    const latencyMetrics = this.calculateLatencyMetrics(latencyData || []);

    // Calcular métricas de error
    const errorMetrics = this.calculateErrorMetrics(latencyData || []);

    // Usuarios únicos
    const uniqueUsers = new Set(
      (usersData || []).map(u => u.user_id).filter(Boolean)
    ).size;

    const snapshot: Omit<PerformanceSnapshot, 'id' | 'createdAt'> = {
      tenantId: config.tenantId,
      periodStart,
      periodEnd,
      granularity: config.granularity,

      // Feedback
      positiveFeedbackRate: feedbackMetrics.positiveRate,
      negativeFeedbackRate: feedbackMetrics.negativeRate,
      noFeedbackRate: feedbackMetrics.noFeedbackRate,

      // Resolution
      resolutionRate: resolutionMetrics.rate,
      avgTurnsToResolution: resolutionMetrics.avgTurns,

      // Latency
      avgResponseTimeMs: latencyMetrics.avg,
      p50ResponseTimeMs: latencyMetrics.p50,
      p95ResponseTimeMs: latencyMetrics.p95,
      p99ResponseTimeMs: latencyMetrics.p99,

      // Errors
      errorRate: errorMetrics.errorRate,
      timeoutRate: errorMetrics.timeoutRate,

      // Usage
      totalConversations: conversations.length,
      totalMessages: (messagesData || []).length,
      uniqueUsers,
    };

    // Guardar
    const { data: saved, error: saveError } = await supabase
      .from('ai_drift_performance_snapshots')
      .upsert({
        tenant_id: snapshot.tenantId,
        period_start: snapshot.periodStart.toISOString(),
        period_end: snapshot.periodEnd.toISOString(),
        granularity: snapshot.granularity,
        positive_feedback_rate: snapshot.positiveFeedbackRate,
        negative_feedback_rate: snapshot.negativeFeedbackRate,
        no_feedback_rate: snapshot.noFeedbackRate,
        resolution_rate: snapshot.resolutionRate,
        avg_turns_to_resolution: snapshot.avgTurnsToResolution,
        avg_response_time_ms: snapshot.avgResponseTimeMs,
        p50_response_time_ms: snapshot.p50ResponseTimeMs,
        p95_response_time_ms: snapshot.p95ResponseTimeMs,
        p99_response_time_ms: snapshot.p99ResponseTimeMs,
        error_rate: snapshot.errorRate,
        timeout_rate: snapshot.timeoutRate,
        total_conversations: snapshot.totalConversations,
        total_messages: snapshot.totalMessages,
        unique_users: snapshot.uniqueUsers,
      }, {
        onConflict: 'tenant_id,period_start,granularity'
      })
      .select()
      .single();

    if (saveError) {
      console.error('Error saving performance snapshot:', saveError);
      throw saveError;
    }

    return {
      ...snapshot,
      id: saved.id,
      createdAt: new Date(saved.created_at),
    };
  }

  private calculateFeedbackMetrics(
    feedback: Array<{ rating: number }>,
    totalMessages: number
  ): {
    positiveRate: number;
    negativeRate: number;
    noFeedbackRate: number;
  } {
    const positive = feedback.filter(f => f.rating > 0).length;
    const negative = feedback.filter(f => f.rating < 0).length;
    const total = feedback.length;

    return {
      positiveRate: total > 0 ? positive / total : 0,
      negativeRate: total > 0 ? negative / total : 0,
      noFeedbackRate: totalMessages > 0 ? (totalMessages - total) / totalMessages : 1,
    };
  }

  private calculateResolutionMetrics(
    conversations: Array<{ id: string; status: string; metadata: any }>,
    messages: Array<{ conversation_id: string }>
  ): {
    rate: number;
    avgTurns: number;
  } {
    const resolved = conversations.filter(c =>
      c.status === 'resolved' ||
      c.metadata?.resolved === true
    ).length;

    // Contar mensajes por conversación
    const messagesByConversation = new Map<string, number>();
    for (const msg of messages) {
      const count = messagesByConversation.get(msg.conversation_id) || 0;
      messagesByConversation.set(msg.conversation_id, count + 1);
    }

    const turnCounts = Array.from(messagesByConversation.values());
    const avgTurns = turnCounts.length > 0
      ? turnCounts.reduce((a, b) => a + b, 0) / turnCounts.length
      : 0;

    return {
      rate: conversations.length > 0 ? resolved / conversations.length : 0,
      avgTurns,
    };
  }

  private calculateLatencyMetrics(
    responses: Array<{ metadata: any }>
  ): {
    avg: number;
    p50: number;
    p95: number;
    p99: number;
  } {
    const latencies = responses
      .map(r => r.metadata?.response_time_ms || r.metadata?.latency_ms)
      .filter((l): l is number => typeof l === 'number')
      .sort((a, b) => a - b);

    if (latencies.length === 0) {
      return { avg: 0, p50: 0, p95: 0, p99: 0 };
    }

    const avg = latencies.reduce((a, b) => a + b, 0) / latencies.length;
    const p50 = this.percentile(latencies, 50);
    const p95 = this.percentile(latencies, 95);
    const p99 = this.percentile(latencies, 99);

    return { avg, p50, p95, p99 };
  }

  private percentile(sortedArray: number[], p: number): number {
    const index = Math.ceil((p / 100) * sortedArray.length) - 1;
    return sortedArray[Math.max(0, index)];
  }

  private calculateErrorMetrics(
    responses: Array<{ metadata: any }>
  ): {
    errorRate: number;
    timeoutRate: number;
  } {
    const errors = responses.filter(r =>
      r.metadata?.error === true ||
      r.metadata?.status === 'error'
    ).length;

    const timeouts = responses.filter(r =>
      r.metadata?.timeout === true ||
      r.metadata?.status === 'timeout'
    ).length;

    return {
      errorRate: responses.length > 0 ? errors / responses.length : 0,
      timeoutRate: responses.length > 0 ? timeouts / responses.length : 0,
    };
  }
}
```

## Orquestador de Colección

```typescript
// lib/ai-learning/drift/collectors/collector-orchestrator.ts

import { createClient } from '@/lib/supabase/server';
import { InputCollector } from './input-collector';
import { OutputCollector } from './output-collector';
import { PerformanceCollector } from './performance-collector';
import type { CollectorConfig } from '../types';

interface CollectionResult {
  tenantId: string;
  periodStart: Date;
  periodEnd: Date;
  inputSnapshot: boolean;
  outputSnapshot: boolean;
  performanceSnapshot: boolean;
  errors: string[];
}

export class CollectorOrchestrator {
  private inputCollector: InputCollector;
  private outputCollector: OutputCollector;
  private performanceCollector: PerformanceCollector;

  constructor() {
    this.inputCollector = new InputCollector();
    this.outputCollector = new OutputCollector();
    this.performanceCollector = new PerformanceCollector();
  }

  /**
   * Ejecuta colección para todos los tenants activos
   */
  async collectAllTenants(
    granularity: 'hourly' | 'daily' | 'weekly'
  ): Promise<CollectionResult[]> {
    const supabase = await createClient();

    // Obtener tenants activos
    const { data: tenants } = await supabase
      .from('tenants')
      .select('id, settings')
      .eq('status', 'active');

    if (!tenants) return [];

    const results: CollectionResult[] = [];
    const { periodStart, periodEnd } = this.calculatePeriod(granularity);

    for (const tenant of tenants) {
      const config: CollectorConfig = {
        tenantId: tenant.id,
        granularity,
        collectEmbeddings: tenant.settings?.ai_learning?.drift_embeddings ?? true,
        collectTokens: tenant.settings?.ai_learning?.drift_tokens ?? true,
        samplingRate: tenant.settings?.ai_learning?.drift_sampling ?? 1.0,
        retentionDays: tenant.settings?.ai_learning?.drift_retention ?? 90,
      };

      const result = await this.collectForTenant(config, periodStart, periodEnd);
      results.push(result);
    }

    return results;
  }

  /**
   * Ejecuta colección para un tenant específico
   */
  async collectForTenant(
    config: CollectorConfig,
    periodStart: Date,
    periodEnd: Date
  ): Promise<CollectionResult> {
    const result: CollectionResult = {
      tenantId: config.tenantId,
      periodStart,
      periodEnd,
      inputSnapshot: false,
      outputSnapshot: false,
      performanceSnapshot: false,
      errors: [],
    };

    // Ejecutar colectores en paralelo
    const [inputResult, outputResult, perfResult] = await Promise.allSettled([
      this.inputCollector.collectForPeriod(config, periodStart, periodEnd),
      this.outputCollector.collectForPeriod(config, periodStart, periodEnd),
      this.performanceCollector.collectForPeriod(config, periodStart, periodEnd),
    ]);

    // Procesar resultados
    if (inputResult.status === 'fulfilled' && inputResult.value) {
      result.inputSnapshot = true;
    } else if (inputResult.status === 'rejected') {
      result.errors.push(`Input: ${inputResult.reason}`);
    }

    if (outputResult.status === 'fulfilled' && outputResult.value) {
      result.outputSnapshot = true;
    } else if (outputResult.status === 'rejected') {
      result.errors.push(`Output: ${outputResult.reason}`);
    }

    if (perfResult.status === 'fulfilled' && perfResult.value) {
      result.performanceSnapshot = true;
    } else if (perfResult.status === 'rejected') {
      result.errors.push(`Performance: ${perfResult.reason}`);
    }

    return result;
  }

  /**
   * Calcula el período basado en granularidad
   */
  private calculatePeriod(granularity: 'hourly' | 'daily' | 'weekly'): {
    periodStart: Date;
    periodEnd: Date;
  } {
    const now = new Date();
    let periodStart: Date;
    let periodEnd: Date;

    switch (granularity) {
      case 'hourly':
        // Hora anterior completa
        periodEnd = new Date(now);
        periodEnd.setMinutes(0, 0, 0);
        periodStart = new Date(periodEnd);
        periodStart.setHours(periodStart.getHours() - 1);
        break;

      case 'daily':
        // Día anterior completo
        periodEnd = new Date(now);
        periodEnd.setHours(0, 0, 0, 0);
        periodStart = new Date(periodEnd);
        periodStart.setDate(periodStart.getDate() - 1);
        break;

      case 'weekly':
        // Semana anterior completa (lunes a domingo)
        periodEnd = new Date(now);
        periodEnd.setHours(0, 0, 0, 0);
        const dayOfWeek = periodEnd.getDay();
        const daysToMonday = dayOfWeek === 0 ? 6 : dayOfWeek - 1;
        periodEnd.setDate(periodEnd.getDate() - daysToMonday);
        periodStart = new Date(periodEnd);
        periodStart.setDate(periodStart.getDate() - 7);
        break;
    }

    return { periodStart, periodEnd };
  }

  /**
   * Limpia snapshots antiguos según retención
   */
  async cleanupOldSnapshots(tenantId: string, retentionDays: number): Promise<void> {
    const supabase = await createClient();
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - retentionDays);

    await Promise.all([
      supabase
        .from('ai_drift_input_snapshots')
        .delete()
        .eq('tenant_id', tenantId)
        .lt('period_end', cutoffDate.toISOString()),

      supabase
        .from('ai_drift_output_snapshots')
        .delete()
        .eq('tenant_id', tenantId)
        .lt('period_end', cutoffDate.toISOString()),

      supabase
        .from('ai_drift_performance_snapshots')
        .delete()
        .eq('tenant_id', tenantId)
        .lt('period_end', cutoffDate.toISOString()),
    ]);
  }
}
```

## API Endpoint para Cron

```typescript
// app/api/cron/drift-collection/route.ts

import { NextRequest, NextResponse } from 'next/server';
import { CollectorOrchestrator } from '@/lib/ai-learning/drift/collectors/collector-orchestrator';

// Vercel Cron: Ejecutar cada hora
export const runtime = 'nodejs';
export const maxDuration = 300; // 5 minutos

export async function GET(request: NextRequest) {
  // Verificar cron secret
  const authHeader = request.headers.get('authorization');
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  const orchestrator = new CollectorOrchestrator();

  try {
    // Determinar granularidad basada en hora actual
    const now = new Date();
    const hour = now.getHours();
    const dayOfWeek = now.getDay();

    // Siempre ejecutar hourly
    const hourlyResults = await orchestrator.collectAllTenants('hourly');

    // Daily a las 00:00
    let dailyResults = null;
    if (hour === 0) {
      dailyResults = await orchestrator.collectAllTenants('daily');
    }

    // Weekly los lunes a las 00:00
    let weeklyResults = null;
    if (hour === 0 && dayOfWeek === 1) {
      weeklyResults = await orchestrator.collectAllTenants('weekly');
    }

    return NextResponse.json({
      success: true,
      timestamp: now.toISOString(),
      results: {
        hourly: {
          count: hourlyResults.length,
          errors: hourlyResults.filter(r => r.errors.length > 0).length,
        },
        daily: dailyResults ? {
          count: dailyResults.length,
          errors: dailyResults.filter(r => r.errors.length > 0).length,
        } : null,
        weekly: weeklyResults ? {
          count: weeklyResults.length,
          errors: weeklyResults.filter(r => r.errors.length > 0).length,
        } : null,
      },
    });
  } catch (error) {
    console.error('Drift collection cron error:', error);
    return NextResponse.json(
      { error: 'Collection failed', details: String(error) },
      { status: 500 }
    );
  }
}
```

## Configuración de Vercel Cron

```json
// vercel.json (agregar a crons existentes)
{
  "crons": [
    {
      "path": "/api/cron/drift-collection",
      "schedule": "0 * * * *"
    }
  ]
}
```

## Tokenizer Utility

```typescript
// lib/ai-learning/utils/tokenizer.ts

export class Tokenizer {
  private stopWords: Set<string>;

  constructor() {
    // Stop words en español e inglés
    this.stopWords = new Set([
      // Español
      'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'de', 'del',
      'a', 'al', 'en', 'con', 'por', 'para', 'y', 'o', 'que', 'es', 'son',
      'se', 'su', 'sus', 'me', 'te', 'le', 'nos', 'les', 'lo', 'mi', 'tu',
      'este', 'esta', 'estos', 'estas', 'ese', 'esa', 'esos', 'esas',
      'como', 'pero', 'si', 'no', 'más', 'muy', 'ya', 'también', 'solo',
      // Inglés
      'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
      'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
      'should', 'may', 'might', 'must', 'can', 'to', 'of', 'in', 'for',
      'on', 'with', 'at', 'by', 'from', 'as', 'or', 'and', 'but', 'if',
      'this', 'that', 'these', 'those', 'it', 'its', 'i', 'you', 'he',
      'she', 'we', 'they', 'what', 'which', 'who', 'when', 'where', 'how',
    ]);
  }

  /**
   * Tokeniza texto en palabras limpias
   */
  tokenize(text: string, removeStopWords = true): string[] {
    // Normalizar y limpiar
    const normalized = text
      .toLowerCase()
      .normalize('NFD')
      .replace(/[\u0300-\u036f]/g, '') // Remover acentos
      .replace(/[^a-z0-9\s]/g, ' ') // Solo alfanuméricos
      .replace(/\s+/g, ' ')
      .trim();

    let tokens = normalized.split(' ').filter(t => t.length > 1);

    if (removeStopWords) {
      tokens = tokens.filter(t => !this.stopWords.has(t));
    }

    return tokens;
  }

  /**
   * Cuenta frecuencia de tokens
   */
  countTokens(tokens: string[]): Map<string, number> {
    const counts = new Map<string, number>();

    for (const token of tokens) {
      counts.set(token, (counts.get(token) || 0) + 1);
    }

    return counts;
  }

  /**
   * Genera n-gramas
   */
  ngrams(tokens: string[], n: number): string[] {
    const result: string[] = [];

    for (let i = 0; i <= tokens.length - n; i++) {
      result.push(tokens.slice(i, i + n).join(' '));
    }

    return result;
  }
}
```

## Siguiente Documento

Continúa con [3.2-STATISTICAL-TESTS.md](./3.2-STATISTICAL-TESTS.md) para la implementación de tests estadísticos de detección de drift.
