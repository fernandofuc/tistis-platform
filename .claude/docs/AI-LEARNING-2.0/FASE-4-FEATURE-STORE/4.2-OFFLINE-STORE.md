# 4.2 Offline Store - Feature Store

## Descripción General

El Offline Store almacena datos históricos de features para:
- Training de modelos (batch queries)
- Point-in-time queries (evitar data leakage)
- Análisis histórico y debugging
- Backfilling de features

## Arquitectura

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          OFFLINE STORE                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │                    WRITE PATH                                       │    │
│  │  ┌──────────┐   ┌──────────────┐   ┌────────────────────────────┐ │    │
│  │  │ Feature  │──▶│ Validation   │──▶│ Partitioned Write          │ │    │
│  │  │ Values   │   │ & Transform  │   │ (Monthly Partitions)       │ │    │
│  │  └──────────┘   └──────────────┘   └────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │                    READ PATH                                        │    │
│  │                                                                     │    │
│  │  Point-in-Time Query     Batch Query        Historical Query       │    │
│  │  ┌──────────────────┐   ┌──────────────┐   ┌──────────────────┐   │    │
│  │  │ AS-OF JOIN       │   │ Time Range   │   │ Full History     │   │    │
│  │  │ Latest before T  │   │ Aggregations │   │ For Entity       │   │    │
│  │  └──────────────────┘   └──────────────┘   └──────────────────┘   │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Offline Store Service

```typescript
// lib/ai-learning/feature-store/offline-store.ts

import { createClient } from '@/lib/supabase/server';
import { FeatureRegistry } from './registry';
import type {
  FeatureValue,
  FeatureEmbedding,
  BatchFeatureRequest,
  BatchFeatureResponse,
  PointInTimeRequest,
  EntityType,
} from './types';

export class OfflineStore {
  private registry: FeatureRegistry;

  constructor() {
    this.registry = new FeatureRegistry();
  }

  // ==========================================
  // WRITE OPERATIONS
  // ==========================================

  /**
   * Escribe un valor de feature al offline store
   */
  async writeFeatureValue(
    tenantId: string,
    featureId: string,
    entityType: EntityType,
    entityId: string,
    value: any,
    eventTimestamp: Date
  ): Promise<void> {
    const supabase = await createClient();

    // Validar feature
    const feature = await this.registry.getFeature(featureId);
    if (!feature) {
      throw new Error(`Feature not found: ${featureId}`);
    }

    // Validar valor
    const validatedValue = this.validateValue(value, feature.validationRules);

    // Determinar tipo de valor
    const valueType = this.determineValueType(value, feature.dataType);

    // Calcular partition key
    const partitionKey = this.getPartitionKey(eventTimestamp);

    await supabase.from('ai_feature_values').insert({
      tenant_id: tenantId,
      feature_id: featureId,
      entity_type: entityType,
      entity_id: entityId,
      value: JSON.stringify(validatedValue),
      value_type: valueType,
      event_timestamp: eventTimestamp.toISOString(),
      partition_key: partitionKey,
    });
  }

  /**
   * Escribe múltiples valores en batch
   */
  async writeBatch(
    tenantId: string,
    values: Array<{
      featureId: string;
      entityType: EntityType;
      entityId: string;
      value: any;
      eventTimestamp: Date;
    }>
  ): Promise<{ written: number; failed: number; errors: string[] }> {
    const supabase = await createClient();
    const errors: string[] = [];
    let written = 0;
    let failed = 0;

    // Agrupar por partition para eficiencia
    const byPartition = new Map<string, typeof values>();

    for (const v of values) {
      const partition = this.getPartitionKey(v.eventTimestamp);
      if (!byPartition.has(partition)) {
        byPartition.set(partition, []);
      }
      byPartition.get(partition)!.push(v);
    }

    // Escribir por partición
    for (const [partition, partitionValues] of byPartition) {
      try {
        const rows = partitionValues.map(v => ({
          tenant_id: tenantId,
          feature_id: v.featureId,
          entity_type: v.entityType,
          entity_id: v.entityId,
          value: JSON.stringify(v.value),
          value_type: this.determineValueType(v.value, 'auto'),
          event_timestamp: v.eventTimestamp.toISOString(),
          partition_key: partition,
        }));

        const { error } = await supabase
          .from('ai_feature_values')
          .insert(rows);

        if (error) {
          errors.push(`Partition ${partition}: ${error.message}`);
          failed += partitionValues.length;
        } else {
          written += partitionValues.length;
        }
      } catch (err) {
        errors.push(`Partition ${partition}: ${err}`);
        failed += partitionValues.length;
      }
    }

    return { written, failed, errors };
  }

  /**
   * Escribe un embedding al offline store
   */
  async writeEmbedding(
    tenantId: string,
    featureId: string,
    entityType: EntityType,
    entityId: string,
    embedding: number[],
    eventTimestamp: Date,
    modelVersion = 'text-embedding-3-small'
  ): Promise<void> {
    const supabase = await createClient();

    await supabase.from('ai_feature_embeddings').insert({
      tenant_id: tenantId,
      feature_id: featureId,
      entity_type: entityType,
      entity_id: entityId,
      embedding: `[${embedding.join(',')}]`,
      model_version: modelVersion,
      event_timestamp: eventTimestamp.toISOString(),
    });
  }

  // ==========================================
  // READ OPERATIONS
  // ==========================================

  /**
   * Obtiene el valor más reciente de un feature para una entidad
   * antes de un timestamp específico (point-in-time)
   */
  async getFeatureAtTime(
    tenantId: string,
    featureId: string,
    entityType: EntityType,
    entityId: string,
    asOfTime: Date
  ): Promise<FeatureValue | null> {
    const supabase = await createClient();

    const { data, error } = await supabase
      .from('ai_feature_values')
      .select('*')
      .eq('tenant_id', tenantId)
      .eq('feature_id', featureId)
      .eq('entity_type', entityType)
      .eq('entity_id', entityId)
      .lte('event_timestamp', asOfTime.toISOString())
      .order('event_timestamp', { ascending: false })
      .limit(1)
      .single();

    if (error) {
      if (error.code === 'PGRST116') return null;
      throw error;
    }

    return this.mapFeatureValue(data);
  }

  /**
   * Obtiene múltiples features para una entidad en un punto en el tiempo
   */
  async getFeaturesAtTime(
    tenantId: string,
    featureIds: string[],
    entityType: EntityType,
    entityId: string,
    asOfTime: Date
  ): Promise<Record<string, any>> {
    const results: Record<string, any> = {};

    // Usar LATERAL JOIN para eficiencia (una sola query)
    const supabase = await createClient();

    // Para cada feature, obtener el valor más reciente antes del timestamp
    const promises = featureIds.map(async (featureId) => {
      const value = await this.getFeatureAtTime(
        tenantId,
        featureId,
        entityType,
        entityId,
        asOfTime
      );

      if (value) {
        const feature = await this.registry.getFeature(featureId);
        const name = feature?.name || featureId;
        results[name] = value.value;
      }
    });

    await Promise.all(promises);

    return results;
  }

  /**
   * Point-in-time join: obtiene features para múltiples entidades
   * cada una en su propio punto en el tiempo
   */
  async getPointInTime(
    tenantId: string,
    request: PointInTimeRequest
  ): Promise<Array<{
    entityId: string;
    timestamp: Date;
    features: Record<string, any>;
  }>> {
    const results: Array<{
      entityId: string;
      timestamp: Date;
      features: Record<string, any>;
    }> = [];

    // Resolver feature IDs
    const featureIdMap = await this.registry.resolveFeatureIds(
      tenantId,
      request.features
    );

    const featureIds = Array.from(featureIdMap.values());

    // Procesar en batches para evitar queries muy grandes
    const batchSize = 100;
    for (let i = 0; i < request.entityTimestampPairs.length; i += batchSize) {
      const batch = request.entityTimestampPairs.slice(i, i + batchSize);

      const batchPromises = batch.map(async ({ entityId, timestamp }) => {
        const features = await this.getFeaturesAtTime(
          tenantId,
          featureIds,
          request.entityType,
          entityId,
          timestamp
        );

        return { entityId, timestamp, features };
      });

      const batchResults = await Promise.all(batchPromises);
      results.push(...batchResults);
    }

    return results;
  }

  /**
   * Obtiene features en un rango de tiempo para training
   */
  async getBatchFeatures(
    tenantId: string,
    request: BatchFeatureRequest
  ): Promise<BatchFeatureResponse> {
    const startTime = Date.now();
    const supabase = await createClient();

    // Resolver feature IDs
    const featureIdMap = await this.registry.resolveFeatureIds(
      tenantId,
      request.features
    );

    const featureIds = Array.from(featureIdMap.values());
    const featureNameById = new Map(
      Array.from(featureIdMap.entries()).map(([name, id]) => [id, name])
    );

    // Query con filtros
    let query = supabase
      .from('ai_feature_values')
      .select('*')
      .eq('tenant_id', tenantId)
      .eq('entity_type', request.entityType)
      .in('feature_id', featureIds);

    if (request.entityIds && request.entityIds.length > 0) {
      query = query.in('entity_id', request.entityIds);
    }

    if (request.startTime) {
      query = query.gte('event_timestamp', request.startTime.toISOString());
    }

    if (request.endTime) {
      query = query.lt('event_timestamp', request.endTime.toISOString());
    }

    query = query.order('entity_id').order('event_timestamp', { ascending: true });

    const { data, error } = await query;

    if (error) throw error;

    // Agrupar por entidad y timestamp
    const grouped = new Map<string, Map<string, Record<string, any>>>();

    for (const row of data || []) {
      const entityId = row.entity_id;
      const timestamp = row.event_timestamp;
      const key = `${entityId}:${timestamp}`;

      if (!grouped.has(key)) {
        grouped.set(key, new Map());
      }

      const featureName = featureNameById.get(row.feature_id) || row.feature_id;
      grouped.get(key)!.set(featureName, JSON.parse(row.value));
    }

    // Convertir a formato de respuesta
    const rows = Array.from(grouped.entries()).map(([key, features]) => {
      const [entityId, timestamp] = key.split(':');
      return {
        entityId,
        eventTimestamp: new Date(timestamp),
        features: Object.fromEntries(features),
      };
    });

    return {
      rows,
      metadata: {
        totalRows: rows.length,
        latencyMs: Date.now() - startTime,
      },
    };
  }

  /**
   * Obtiene historial completo de un feature para una entidad
   */
  async getFeatureHistory(
    tenantId: string,
    featureId: string,
    entityType: EntityType,
    entityId: string,
    options: {
      startTime?: Date;
      endTime?: Date;
      limit?: number;
    } = {}
  ): Promise<FeatureValue[]> {
    const supabase = await createClient();

    let query = supabase
      .from('ai_feature_values')
      .select('*')
      .eq('tenant_id', tenantId)
      .eq('feature_id', featureId)
      .eq('entity_type', entityType)
      .eq('entity_id', entityId)
      .order('event_timestamp', { ascending: false });

    if (options.startTime) {
      query = query.gte('event_timestamp', options.startTime.toISOString());
    }

    if (options.endTime) {
      query = query.lte('event_timestamp', options.endTime.toISOString());
    }

    if (options.limit) {
      query = query.limit(options.limit);
    }

    const { data, error } = await query;

    if (error) throw error;

    return (data || []).map(this.mapFeatureValue);
  }

  /**
   * Busca embeddings similares
   */
  async searchSimilarEmbeddings(
    tenantId: string,
    featureId: string,
    queryEmbedding: number[],
    options: {
      limit?: number;
      threshold?: number;
      entityType?: EntityType;
    } = {}
  ): Promise<Array<{
    entityId: string;
    similarity: number;
    eventTimestamp: Date;
  }>> {
    const supabase = await createClient();

    const limit = options.limit || 10;
    const threshold = options.threshold || 0.7;

    let query = supabase.rpc('search_feature_embeddings', {
      p_tenant_id: tenantId,
      p_feature_id: featureId,
      p_query_embedding: `[${queryEmbedding.join(',')}]`,
      p_limit: limit,
      p_threshold: threshold,
    });

    if (options.entityType) {
      query = query.eq('entity_type', options.entityType);
    }

    const { data, error } = await query;

    if (error) throw error;

    return (data || []).map((row: any) => ({
      entityId: row.entity_id,
      similarity: row.similarity,
      eventTimestamp: new Date(row.event_timestamp),
    }));
  }

  // ==========================================
  // AGGREGATION OPERATIONS
  // ==========================================

  /**
   * Calcula agregaciones sobre un feature
   */
  async aggregateFeature(
    tenantId: string,
    featureId: string,
    entityType: EntityType,
    options: {
      aggregation: 'sum' | 'avg' | 'count' | 'min' | 'max';
      startTime?: Date;
      endTime?: Date;
      groupBy?: 'hour' | 'day' | 'week' | 'month';
      entityIds?: string[];
    }
  ): Promise<Array<{
    period?: string;
    entityId?: string;
    value: number;
  }>> {
    const supabase = await createClient();

    // Construir query de agregación
    const aggFunc = options.aggregation.toUpperCase();
    const groupByClause = options.groupBy
      ? `DATE_TRUNC('${options.groupBy}', event_timestamp)`
      : null;

    // Usar RPC para query customizada
    const { data, error } = await supabase.rpc('aggregate_feature_values', {
      p_tenant_id: tenantId,
      p_feature_id: featureId,
      p_entity_type: entityType,
      p_aggregation: options.aggregation,
      p_start_time: options.startTime?.toISOString(),
      p_end_time: options.endTime?.toISOString(),
      p_group_by: options.groupBy,
      p_entity_ids: options.entityIds,
    });

    if (error) throw error;

    return data || [];
  }

  // ==========================================
  // MATERIALIZATION
  // ==========================================

  /**
   * Materializa features desde SQL de transformación
   */
  async materializeFeature(
    tenantId: string,
    featureId: string,
    timeRange: { start: Date; end: Date }
  ): Promise<{ rowsWritten: number }> {
    const feature = await this.registry.getFeature(featureId);
    if (!feature) throw new Error(`Feature not found: ${featureId}`);
    if (!feature.transformationSql) {
      throw new Error(`Feature ${featureId} has no transformation SQL`);
    }

    const group = await this.registry.getGroup(feature.groupId);
    if (!group) throw new Error(`Feature group not found`);

    const supabase = await createClient();

    // Ejecutar SQL de transformación con parámetros
    const sql = feature.transformationSql
      .replace(':tenant_id', `'${tenantId}'`)
      .replace(':start_time', `'${timeRange.start.toISOString()}'`)
      .replace(':end_time', `'${timeRange.end.toISOString()}'`);

    const { data, error } = await supabase.rpc('execute_feature_sql', {
      p_sql: sql,
    });

    if (error) throw error;

    // Escribir resultados al offline store
    const values = (data || []).map((row: any) => ({
      featureId: feature.id,
      entityType: group.entityType,
      entityId: row.entity_id,
      value: row.value,
      eventTimestamp: timeRange.end, // Usar fin del rango como timestamp
    }));

    const result = await this.writeBatch(tenantId, values);

    return { rowsWritten: result.written };
  }

  // ==========================================
  // MAINTENANCE
  // ==========================================

  /**
   * Limpia datos antiguos basado en retención
   */
  async cleanup(
    tenantId: string,
    retentionDays: number
  ): Promise<{ deletedRows: number }> {
    const supabase = await createClient();
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - retentionDays);

    const { count, error } = await supabase
      .from('ai_feature_values')
      .delete({ count: 'exact' })
      .eq('tenant_id', tenantId)
      .lt('event_timestamp', cutoffDate.toISOString());

    if (error) throw error;

    return { deletedRows: count || 0 };
  }

  /**
   * Obtiene estadísticas del offline store
   */
  async getStats(tenantId: string): Promise<{
    totalRows: number;
    uniqueFeatures: number;
    oldestTimestamp: Date | null;
    newestTimestamp: Date | null;
    byEntityType: Record<string, number>;
  }> {
    const supabase = await createClient();

    const { data, error } = await supabase.rpc('get_feature_store_stats', {
      p_tenant_id: tenantId,
    });

    if (error) throw error;

    return {
      totalRows: data?.total_rows || 0,
      uniqueFeatures: data?.unique_features || 0,
      oldestTimestamp: data?.oldest_timestamp
        ? new Date(data.oldest_timestamp)
        : null,
      newestTimestamp: data?.newest_timestamp
        ? new Date(data.newest_timestamp)
        : null,
      byEntityType: data?.by_entity_type || {},
    };
  }

  // ==========================================
  // HELPERS
  // ==========================================

  private validateValue(value: any, rules?: Record<string, any>): any {
    if (!rules) return value;

    if (rules.notNull && (value === null || value === undefined)) {
      throw new Error('Value cannot be null');
    }

    if (typeof value === 'number') {
      if (rules.min !== undefined && value < rules.min) {
        throw new Error(`Value ${value} is below minimum ${rules.min}`);
      }
      if (rules.max !== undefined && value > rules.max) {
        throw new Error(`Value ${value} is above maximum ${rules.max}`);
      }
    }

    if (rules.allowedValues && !rules.allowedValues.includes(value)) {
      throw new Error(`Value ${value} is not in allowed values`);
    }

    if (rules.pattern && typeof value === 'string') {
      if (!new RegExp(rules.pattern).test(value)) {
        throw new Error(`Value does not match pattern ${rules.pattern}`);
      }
    }

    return value;
  }

  private determineValueType(value: any, dataType: string): string {
    if (Array.isArray(value)) {
      if (value.length > 0 && typeof value[0] === 'number' && value.length > 100) {
        return 'embedding';
      }
      return 'vector';
    }
    return 'scalar';
  }

  private getPartitionKey(timestamp: Date): string {
    return timestamp.toISOString().slice(0, 10); // YYYY-MM-DD
  }

  private mapFeatureValue(row: any): FeatureValue {
    return {
      id: row.id,
      tenantId: row.tenant_id,
      featureId: row.feature_id,
      entityType: row.entity_type,
      entityId: row.entity_id,
      value: JSON.parse(row.value),
      valueType: row.value_type,
      eventTimestamp: new Date(row.event_timestamp),
      createdAt: new Date(row.created_at),
    };
  }
}
```

## SQL Functions para Operaciones Avanzadas

```sql
-- Function para búsqueda de embeddings similares
CREATE OR REPLACE FUNCTION search_feature_embeddings(
  p_tenant_id UUID,
  p_feature_id UUID,
  p_query_embedding vector(1536),
  p_limit INT DEFAULT 10,
  p_threshold FLOAT DEFAULT 0.7
)
RETURNS TABLE (
  entity_id TEXT,
  entity_type TEXT,
  similarity FLOAT,
  event_timestamp TIMESTAMPTZ
) AS $$
BEGIN
  RETURN QUERY
  SELECT
    fe.entity_id,
    fe.entity_type,
    1 - (fe.embedding <=> p_query_embedding) as similarity,
    fe.event_timestamp
  FROM ai_feature_embeddings fe
  WHERE fe.tenant_id = p_tenant_id
    AND fe.feature_id = p_feature_id
    AND 1 - (fe.embedding <=> p_query_embedding) >= p_threshold
  ORDER BY fe.embedding <=> p_query_embedding
  LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;

-- Function para agregaciones de features
CREATE OR REPLACE FUNCTION aggregate_feature_values(
  p_tenant_id UUID,
  p_feature_id UUID,
  p_entity_type TEXT,
  p_aggregation TEXT,
  p_start_time TIMESTAMPTZ DEFAULT NULL,
  p_end_time TIMESTAMPTZ DEFAULT NULL,
  p_group_by TEXT DEFAULT NULL,
  p_entity_ids TEXT[] DEFAULT NULL
)
RETURNS TABLE (
  period TEXT,
  entity_id TEXT,
  agg_value FLOAT
) AS $$
DECLARE
  v_sql TEXT;
  v_agg_func TEXT;
  v_group_clause TEXT := '';
  v_where_clause TEXT := '';
BEGIN
  -- Determinar función de agregación
  v_agg_func := CASE p_aggregation
    WHEN 'sum' THEN 'SUM((value::jsonb)::text::float)'
    WHEN 'avg' THEN 'AVG((value::jsonb)::text::float)'
    WHEN 'count' THEN 'COUNT(*)'
    WHEN 'min' THEN 'MIN((value::jsonb)::text::float)'
    WHEN 'max' THEN 'MAX((value::jsonb)::text::float)'
    ELSE 'COUNT(*)'
  END;

  -- Construir cláusula WHERE
  v_where_clause := format('tenant_id = %L AND feature_id = %L AND entity_type = %L',
    p_tenant_id, p_feature_id, p_entity_type);

  IF p_start_time IS NOT NULL THEN
    v_where_clause := v_where_clause || format(' AND event_timestamp >= %L', p_start_time);
  END IF;

  IF p_end_time IS NOT NULL THEN
    v_where_clause := v_where_clause || format(' AND event_timestamp < %L', p_end_time);
  END IF;

  IF p_entity_ids IS NOT NULL AND array_length(p_entity_ids, 1) > 0 THEN
    v_where_clause := v_where_clause || format(' AND entity_id = ANY(%L)', p_entity_ids);
  END IF;

  -- Construir cláusula GROUP BY
  IF p_group_by IS NOT NULL THEN
    v_group_clause := format('DATE_TRUNC(%L, event_timestamp)::text as period, ', p_group_by);
  ELSE
    v_group_clause := 'NULL::text as period, ';
  END IF;

  -- Construir y ejecutar query
  v_sql := format('
    SELECT
      %s
      entity_id,
      %s as agg_value
    FROM ai_feature_values
    WHERE %s
    GROUP BY %s entity_id
    ORDER BY period, entity_id
  ', v_group_clause, v_agg_func, v_where_clause,
     CASE WHEN p_group_by IS NOT NULL THEN format('DATE_TRUNC(%L, event_timestamp), ', p_group_by) ELSE '' END);

  RETURN QUERY EXECUTE v_sql;
END;
$$ LANGUAGE plpgsql;

-- Function para estadísticas del store
CREATE OR REPLACE FUNCTION get_feature_store_stats(p_tenant_id UUID)
RETURNS TABLE (
  total_rows BIGINT,
  unique_features INT,
  oldest_timestamp TIMESTAMPTZ,
  newest_timestamp TIMESTAMPTZ,
  by_entity_type JSONB
) AS $$
BEGIN
  RETURN QUERY
  SELECT
    COUNT(*)::bigint as total_rows,
    COUNT(DISTINCT feature_id)::int as unique_features,
    MIN(event_timestamp) as oldest_timestamp,
    MAX(event_timestamp) as newest_timestamp,
    jsonb_object_agg(entity_type, cnt) as by_entity_type
  FROM (
    SELECT
      entity_type,
      COUNT(*) as cnt
    FROM ai_feature_values
    WHERE tenant_id = p_tenant_id
    GROUP BY entity_type
  ) sub
  CROSS JOIN (
    SELECT
      COUNT(*) as total,
      COUNT(DISTINCT feature_id) as features,
      MIN(event_timestamp) as min_ts,
      MAX(event_timestamp) as max_ts
    FROM ai_feature_values
    WHERE tenant_id = p_tenant_id
  ) totals;
END;
$$ LANGUAGE plpgsql;

-- Function para ejecutar SQL de transformación de features
CREATE OR REPLACE FUNCTION execute_feature_sql(p_sql TEXT)
RETURNS TABLE (
  entity_id TEXT,
  value JSONB
) AS $$
BEGIN
  RETURN QUERY EXECUTE p_sql;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Limitar acceso a la función de ejecución de SQL
REVOKE ALL ON FUNCTION execute_feature_sql FROM PUBLIC;
GRANT EXECUTE ON FUNCTION execute_feature_sql TO authenticated;
```

## Data Loader para Training

```typescript
// lib/ai-learning/feature-store/data-loader.ts

import { OfflineStore } from './offline-store';
import type {
  EntityType,
  PointInTimeRequest,
  BatchFeatureRequest,
} from './types';

interface TrainingDataConfig {
  tenantId: string;
  entityType: EntityType;
  features: string[];
  labelColumn?: string;
  timeRange?: { start: Date; end: Date };
  sampleSize?: number;
  stratifyBy?: string;
}

interface TrainingDataset {
  X: number[][];
  y?: number[];
  featureNames: string[];
  entityIds: string[];
  timestamps: Date[];
  metadata: {
    totalSamples: number;
    featureCount: number;
    timeRange: { start: Date; end: Date };
    missingValues: Record<string, number>;
  };
}

export class FeatureDataLoader {
  private offlineStore: OfflineStore;

  constructor() {
    this.offlineStore = new OfflineStore();
  }

  /**
   * Carga datos de training con point-in-time correctness
   */
  async loadTrainingData(config: TrainingDataConfig): Promise<TrainingDataset> {
    // Obtener entidades y sus timestamps para training
    const entityTimestamps = await this.getEntityTimestamps(config);

    // Point-in-time query para evitar data leakage
    const pitRequest: PointInTimeRequest = {
      entityType: config.entityType,
      entityTimestampPairs: entityTimestamps,
      features: config.features,
    };

    const results = await this.offlineStore.getPointInTime(
      config.tenantId,
      pitRequest
    );

    // Convertir a formato de dataset
    const featureNames = config.features;
    const X: number[][] = [];
    const y: number[] = [];
    const entityIds: string[] = [];
    const timestamps: Date[] = [];
    const missingValues: Record<string, number> = {};

    // Inicializar contadores de missing
    for (const name of featureNames) {
      missingValues[name] = 0;
    }

    for (const row of results) {
      const features: number[] = [];

      for (const name of featureNames) {
        const value = row.features[name];

        if (value === null || value === undefined) {
          features.push(0); // Default value
          missingValues[name]++;
        } else if (typeof value === 'number') {
          features.push(value);
        } else if (typeof value === 'boolean') {
          features.push(value ? 1 : 0);
        } else if (Array.isArray(value)) {
          features.push(...value.map(v => Number(v) || 0));
        } else {
          features.push(0); // Non-numeric
        }
      }

      X.push(features);
      entityIds.push(row.entityId);
      timestamps.push(row.timestamp);

      // Si hay label column, extraerlo
      if (config.labelColumn && row.features[config.labelColumn] !== undefined) {
        y.push(Number(row.features[config.labelColumn]));
      }
    }

    // Aplicar sampling si es necesario
    let finalX = X;
    let finalY = y;
    let finalEntityIds = entityIds;
    let finalTimestamps = timestamps;

    if (config.sampleSize && config.sampleSize < X.length) {
      const indices = this.sampleIndices(
        X.length,
        config.sampleSize,
        config.stratifyBy ? y : undefined
      );

      finalX = indices.map(i => X[i]);
      finalY = indices.map(i => y[i]);
      finalEntityIds = indices.map(i => entityIds[i]);
      finalTimestamps = indices.map(i => timestamps[i]);
    }

    return {
      X: finalX,
      y: finalY.length > 0 ? finalY : undefined,
      featureNames,
      entityIds: finalEntityIds,
      timestamps: finalTimestamps,
      metadata: {
        totalSamples: finalX.length,
        featureCount: featureNames.length,
        timeRange: {
          start: new Date(Math.min(...finalTimestamps.map(t => t.getTime()))),
          end: new Date(Math.max(...finalTimestamps.map(t => t.getTime()))),
        },
        missingValues,
      },
    };
  }

  /**
   * Carga datos en formato para JSONL (fine-tuning)
   */
  async loadForFineTuning(
    config: TrainingDataConfig & {
      inputFeatures: string[];
      outputFeature: string;
      formatFn?: (input: Record<string, any>, output: any) => {
        messages: Array<{ role: string; content: string }>;
      };
    }
  ): Promise<Array<{ messages: Array<{ role: string; content: string }> }>> {
    const allFeatures = [...config.inputFeatures, config.outputFeature];

    const dataset = await this.loadTrainingData({
      ...config,
      features: allFeatures,
    });

    const samples: Array<{ messages: Array<{ role: string; content: string }> }> = [];

    for (let i = 0; i < dataset.X.length; i++) {
      const inputValues: Record<string, any> = {};
      let outputValue: any;

      for (let j = 0; j < dataset.featureNames.length; j++) {
        const name = dataset.featureNames[j];
        const value = dataset.X[i][j];

        if (name === config.outputFeature) {
          outputValue = value;
        } else if (config.inputFeatures.includes(name)) {
          inputValues[name] = value;
        }
      }

      if (config.formatFn) {
        samples.push(config.formatFn(inputValues, outputValue));
      } else {
        // Default format
        samples.push({
          messages: [
            {
              role: 'system',
              content: 'You are a helpful AI assistant.',
            },
            {
              role: 'user',
              content: JSON.stringify(inputValues),
            },
            {
              role: 'assistant',
              content: String(outputValue),
            },
          ],
        });
      }
    }

    return samples;
  }

  /**
   * Exporta dataset a CSV
   */
  async exportToCSV(config: TrainingDataConfig): Promise<string> {
    const dataset = await this.loadTrainingData(config);

    const headers = ['entity_id', 'timestamp', ...dataset.featureNames];
    if (dataset.y) headers.push('label');

    const rows = [headers.join(',')];

    for (let i = 0; i < dataset.X.length; i++) {
      const row = [
        dataset.entityIds[i],
        dataset.timestamps[i].toISOString(),
        ...dataset.X[i].map(String),
      ];

      if (dataset.y) {
        row.push(String(dataset.y[i]));
      }

      rows.push(row.join(','));
    }

    return rows.join('\n');
  }

  /**
   * Obtiene pares entidad-timestamp para training
   */
  private async getEntityTimestamps(
    config: TrainingDataConfig
  ): Promise<Array<{ entityId: string; timestamp: Date }>> {
    // En una implementación real, esto vendría de la tabla de eventos
    // Por ahora, simulamos obteniendo de las conversaciones

    const supabase = await createClient();

    let query = supabase
      .from('ai_conversations')
      .select('id, user_id, created_at')
      .eq('tenant_id', config.tenantId);

    if (config.timeRange) {
      query = query
        .gte('created_at', config.timeRange.start.toISOString())
        .lt('created_at', config.timeRange.end.toISOString());
    }

    const { data } = await query;

    return (data || []).map(row => ({
      entityId: config.entityType === 'conversation' ? row.id : row.user_id,
      timestamp: new Date(row.created_at),
    }));
  }

  /**
   * Sampling con soporte para estratificación
   */
  private sampleIndices(
    total: number,
    sampleSize: number,
    stratifyLabels?: number[]
  ): number[] {
    if (!stratifyLabels) {
      // Random sampling
      const indices = Array.from({ length: total }, (_, i) => i);
      this.shuffle(indices);
      return indices.slice(0, sampleSize);
    }

    // Stratified sampling
    const byLabel = new Map<number, number[]>();

    for (let i = 0; i < stratifyLabels.length; i++) {
      const label = stratifyLabels[i];
      if (!byLabel.has(label)) {
        byLabel.set(label, []);
      }
      byLabel.get(label)!.push(i);
    }

    const sampledIndices: number[] = [];
    const labelCounts = Array.from(byLabel.entries());
    const totalLabeled = labelCounts.reduce((sum, [, indices]) => sum + indices.length, 0);

    for (const [label, indices] of labelCounts) {
      const proportion = indices.length / totalLabeled;
      const labelSampleSize = Math.round(sampleSize * proportion);

      this.shuffle(indices);
      sampledIndices.push(...indices.slice(0, labelSampleSize));
    }

    this.shuffle(sampledIndices);
    return sampledIndices;
  }

  private shuffle(array: any[]): void {
    for (let i = array.length - 1; i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      [array[i], array[j]] = [array[j], array[i]];
    }
  }
}

// Import necesario
import { createClient } from '@/lib/supabase/server';
```

## Siguiente Documento

Continúa con [4.3-ONLINE-STORE.md](./4.3-ONLINE-STORE.md) para la implementación del Online Store.
